{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViLT Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# ViltModel is a raw model with no heads. Can use to define heads\n",
    "from transformers import ViltProcessor, ViltModel\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "import skimage\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import FloatProgress\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViltImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"size_divisor\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_pad\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViltImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 384\n",
       "  },\n",
       "  \"size_divisor\": 32\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 6057, 2033, 4168, 102]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed = processor(Image.open('./dataset/img/98724.png').convert('RGB'),'funny meme')\n",
    "test_processed['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViltModel(\n",
       "  (embeddings): ViltEmbeddings(\n",
       "    (text_embeddings): TextEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(40, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (patch_embeddings): ViltPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "    )\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViltEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViltLayer(\n",
       "        (attention): ViltAttention(\n",
       "          (attention): ViltSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViltSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViltIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViltOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViltPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No separate text and visual encoders, combbined into one transformer encoder that processes both text and image patches in ViT style\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model with Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViltForClassification(nn.Module):\n",
    "    def __init__(self, vilt_base_model, device):\n",
    "        super().__init__()\n",
    "        self.vilt = vilt_base_model\n",
    "        self.device = device\n",
    "        self.vilt.to(self.device)\n",
    "        # Update all model parameters to be frozen, except for last few layers of certain parts\n",
    "        for param in self.vilt.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze for last layer of encoder\n",
    "        for param in self.vilt.encoder.layer[11].parameters():\n",
    "            param.requires_grad = True\n",
    "        # Unfreeze for final linear pooler layer\n",
    "        for param in self.vilt.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "        # First linear layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear = nn.Linear(768, 38)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        # Final layer, 2 classes hateful or non-hateful\n",
    "        self.linear2 = nn.Linear(38, 2)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Unpack the processed image and text and feed to VILT model\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "        attn_mask = batch['attention_mask'].to(self.device)\n",
    "        pixel_vals = batch['pixel_values'].to(self.device)\n",
    "        pixel_mask = batch['pixel_mask'].to(self.device)\n",
    "        # Remove additional dimension at 1, specify to not remove batch during inference\n",
    "        model_outs = self.vilt(input_ids.squeeze(1), attn_mask.squeeze(1), token_type_ids.squeeze(1), pixel_vals.squeeze(1), pixel_mask.squeeze(1))\n",
    "        # Feed model outs to ReLU\n",
    "        relu_model_outs = self.relu1(model_outs.pooler_output)\n",
    "        # Linear layer and ReLU\n",
    "        output_1 = self.linear(relu_model_outs)\n",
    "        relu_output_1 = self.relu2(output_1)\n",
    "        output = self.linear2(relu_output_1)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, root_dir, jsonl_path, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(self.root_dir, jsonl_path), 'r') as f:\n",
    "            self.jsonl = list(f)\n",
    "        self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.jsonl)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        json_str = self.jsonl[i]\n",
    "        json_loaded = json.loads(json_str)\n",
    "        img_path = os.path.join(self.root_dir, json_loaded['img'])\n",
    "        label = torch.tensor(json_loaded['label'])\n",
    "        caption = json_loaded['text']\n",
    "        img = np.array(Image.open(img_path).convert('RGB')).transpose((2, 0, 1))\n",
    "        img = tv_tensors.Image(img)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        img = np.array(img).transpose((1,2,0))\n",
    "        # Need to truncate, pad to max length to ensure that no dataloader issues happen!\n",
    "        processed = processor(img, caption, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': processed['input_ids'],\n",
    "            'token_type_ids': processed['token_type_ids'],\n",
    "            'attention_mask': processed['attention_mask'],\n",
    "            'pixel_values': processed['pixel_values'],\n",
    "            'pixel_mask': processed['pixel_mask'],\n",
    "            'label':  label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch_sample in tqdm(dataloader):\n",
    "        label = batch_sample['label'].to(device)\n",
    "        pred = model(batch_sample)\n",
    "        loss = loss_fn(pred, label)\n",
    "        # print(total_loss)\n",
    "        correct = (torch.argmax(pred, dim = 1) == label).sum()\n",
    "        acc = correct / label.shape[0]\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Add loss\n",
    "        epoch_loss += loss.item() * label.shape[0]\n",
    "        epoch_acc += acc.item() * label.shape[0]\n",
    "    final_epoch_loss = epoch_loss / len(dataloader.sampler)\n",
    "    final_epoch_acc = epoch_acc / len(dataloader.sampler)\n",
    "    return final_epoch_loss, final_epoch_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune on label directly, ignore caption\n",
    "def finetune_model(model, dataloader, optimizer, loss, epochs, device):\n",
    "    model.to(device)\n",
    "    # Keep track of train metrics\n",
    "    train_loss_ls = []\n",
    "    train_acc_ls = []\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        # Train model\n",
    "        epoch_loss, epoch_acc, model = finetune_one_epoch(model, dataloader, optimizer, loss, device)\n",
    "        train_loss_ls.append(epoch_loss)\n",
    "        train_acc_ls.append(epoch_acc)\n",
    "        print(f\"Epoch {epoch}\\n\")\n",
    "        print(f\"\\ttrain_loss: {epoch_loss}\\n\")\n",
    "        print(f\"\\ttrain_acc: {epoch_acc}\\n\")\n",
    "    return model, train_loss_ls, train_acc_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViltForClassification(\n",
       "  (vilt): ViltModel(\n",
       "    (embeddings): ViltEmbeddings(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768)\n",
       "        (position_embeddings): Embedding(40, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (patch_embeddings): ViltPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViltEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViltPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (relu1): ReLU()\n",
       "  (linear): Linear(in_features=768, out_features=38, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (linear2): Linear(in_features=38, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model = ViltForClassification(model, 'cuda')\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr  = 5e-5)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HatefulMemesDataset('./dataset', 'train.jsonl', transforms=v2.Compose([v2.CenterCrop((800, 800))]))\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [06:08<00:00,  5.50s/it]\n",
      " 10%|█         | 1/10 [06:08<55:14, 368.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "\ttrain_loss: 0.6290790331784417\n",
      "\n",
      "\ttrain_acc: 0.655058823557461\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:10<00:00,  4.63s/it]\n",
      " 20%|██        | 2/10 [11:18<44:33, 334.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "\n",
      "\ttrain_loss: 0.5641836188260246\n",
      "\n",
      "\ttrain_acc: 0.7121176471710206\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:16<00:00,  4.73s/it]\n",
      " 30%|███       | 3/10 [16:35<38:03, 326.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "\n",
      "\ttrain_loss: 0.5215061726009145\n",
      "\n",
      "\ttrain_acc: 0.7436470588235294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:53<00:00,  5.27s/it]\n",
      " 40%|████      | 4/10 [22:28<33:41, 336.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "\n",
      "\ttrain_loss: 0.48792184880200556\n",
      "\n",
      "\ttrain_acc: 0.7732941177312066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:13<00:00,  4.68s/it]\n",
      " 50%|█████     | 5/10 [27:42<27:22, 328.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "\n",
      "\ttrain_loss: 0.45569926343244666\n",
      "\n",
      "\ttrain_acc: 0.7967058826895321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:16<00:00,  4.72s/it]\n",
      " 60%|██████    | 6/10 [32:58<21:37, 324.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "\n",
      "\ttrain_loss: 0.42062176327144396\n",
      "\n",
      "\ttrain_acc: 0.8238823531655705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:02<00:00,  4.51s/it]\n",
      " 70%|███████   | 7/10 [38:00<15:51, 317.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "\n",
      "\ttrain_loss: 0.38380383689263287\n",
      "\n",
      "\ttrain_acc: 0.8494117649302763\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:55<00:00,  4.41s/it]\n",
      " 80%|████████  | 8/10 [42:56<10:20, 310.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "\n",
      "\ttrain_loss: 0.34378311850042903\n",
      "\n",
      "\ttrain_acc: 0.8743529412886676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:58<00:00,  4.45s/it]\n",
      " 90%|█████████ | 9/10 [47:54<05:06, 306.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "\n",
      "\ttrain_loss: 0.3059117962893318\n",
      "\n",
      "\ttrain_acc: 0.8961176472551683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:58<00:00,  4.46s/it]\n",
      "100%|██████████| 10/10 [52:53<00:00, 317.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "\n",
      "\ttrain_loss: 0.2688689673157299\n",
      "\n",
      "\ttrain_acc: 0.9167058826334337\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuned_model, train_losses, train_accs = finetune_model(classification_model, train_dataloader, optimizer, loss, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_capability(model, jsonl_path):\n",
    "    labels = []\n",
    "    model_probs = []\n",
    "    model_preds = []\n",
    "    # 0 -> Non-hateful, 1 -> Hateful\n",
    "    with open(jsonl_path, 'r') as json_f:\n",
    "        json_list = list(json_f)\n",
    "    for json_str in tqdm(json_list):\n",
    "        result = json.loads(json_str)\n",
    "        img_path, label, caption = result['img'], result['label'], result['text']\n",
    "        labels.append(label)\n",
    "        # Read image\n",
    "        batch = processor(Image.open(os.path.join('./dataset', img_path)).convert('RGB'), caption, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            out = model(batch)\n",
    "            probs = out.softmax(dim=-1).cpu().numpy()\n",
    "            class_1_prob = probs[0][1]\n",
    "            model_probs.append(class_1_prob)\n",
    "            model_preds.append(np.argmax(probs))\n",
    "    return roc_auc_score(labels, model_probs), accuracy_score(labels, model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:20<00:00, 24.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6708005952857211, 0.602)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_capability(finetuned_model, './dataset/dev_seen.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hateful_memes_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
