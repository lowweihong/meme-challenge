{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b617ff-8cda-4968-8c1d-bd50d9173d9e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T03:13:43.889389Z",
     "iopub.status.busy": "2024-04-30T03:13:43.888821Z",
     "iopub.status.idle": "2024-04-30T03:13:46.365131Z",
     "shell.execute_reply": "2024-04-30T03:13:46.364330Z",
     "shell.execute_reply.started": "2024-04-30T03:13:43.889352Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate #Params\n",
    "import torch\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb9bbf6-a9ca-4942-9098-8a330a6a2140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T03:13:46.367095Z",
     "iopub.status.busy": "2024-04-30T03:13:46.366568Z",
     "iopub.status.idle": "2024-04-30T03:13:46.374971Z",
     "shell.execute_reply": "2024-04-30T03:13:46.374211Z",
     "shell.execute_reply.started": "2024-04-30T03:13:46.367069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "res_map = {\n",
    "    # 'clip_entire_model_added_sigmoid_gradclip.pt':{\n",
    "    #     'model_':'CLIPModel',\n",
    "    #     'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "    #     'param': {\n",
    "    #         'epo': 15,\n",
    "    #         'head': 'concat',\n",
    "    #         'map_dim': 32,\n",
    "    #         'batch_size': 16,\n",
    "    #         'po_layer': 1\n",
    "    #     }\n",
    "    # },\n",
    "    'clip_entire_model_added_sigmoid_gradclip.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 15,\n",
    "            'head': 'concat',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 1\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip-cross.pt':{ #BEST\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'cross',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 5\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip_laion-CLIP-ViT-B-32-laion2B-s34B-b79K-cross.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'cross',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 5\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip-att-layer5.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'self-att',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 5\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip-cross-layer10.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'cross',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 10\n",
    "        }\n",
    "    },\n",
    "    # 'clip_entire_model_added_sigmoid_gradclip-cross-unfreeze-last-block.pt':{\n",
    "    #     'model_':'CLIPModel',\n",
    "    #     'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "    #     'param': {\n",
    "    #         'epo': 20,\n",
    "    #         'head': 'cross',\n",
    "    #         'map_dim': 1024,\n",
    "    #         'batch_size': 8,\n",
    "    #         'po_layer': 5\n",
    "    #     }\n",
    "    # }\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3347f256-ce34-40c6-893a-7fea21159e56",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-29T08:35:41.894634Z",
     "iopub.status.busy": "2024-04-29T08:35:41.894150Z",
     "iopub.status.idle": "2024-04-29T08:39:03.629314Z",
     "shell.execute_reply": "2024-04-29T08:39:03.628583Z",
     "shell.execute_reply.started": "2024-04-29T08:35:41.894606Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_entire_model_added_sigmoid_gradclip.pt 3,936,257\n",
      "clip_entire_model_added_sigmoid_gradclip-cross.pt 1,075,580,929\n",
      "clip_entire_model_added_sigmoid_gradclip_laion-CLIP-ViT-B-32-laion2B-s34B-b79K-cross.pt 1,075,056,641\n",
      "clip_entire_model_added_sigmoid_gradclip-att-layer5.pt 24,922,113\n",
      "clip_entire_model_added_sigmoid_gradclip-cross-layer10.pt 1,094,473,729\n"
     ]
    }
   ],
   "source": [
    "for fp in res_map.keys():\n",
    "    # print(model)\n",
    "    model = torch.load(f'model_output/{fp}')\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(fp, '{:,}'.format(pytorch_total_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9459e8b1-a987-4a65-b9e6-e0ed56af1f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For BLIP\n",
    "res_map = {\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip-image-captioning-large-inn.pt':{\n",
    "    #     'model_':'BlipModel',\n",
    "    #     'pretrained_model': \"Salesforce/blip-image-captioning-large\",\n",
    "    #     'processer': 'BLIPProcessDataset'\n",
    "    # },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipForImageTextRetrieval-blip-itm-large-coco-new.pt':{\n",
    "    #     'model_': 'BlipForImageTextRetrieval',\n",
    "    #     'pretrained_model': 'Salesforce/blip-itm-large-coco',\n",
    "    #     'processer': 'BLIPProcessDataset'\n",
    "    # },\n",
    "    # 'blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-LR-EPO.pt':{ # BEST BlipModel \"Salesforce/blip-image-captioning-large\"\n",
    "    #     'model_': 'BlipModel', \n",
    "    #     'pretrained_model': \"Salesforce/blip-image-captioning-large\",\n",
    "    #     'processer': 'BLIPProcessDataset'\n",
    "    # },\n",
    "    'blip_entire_model_Salesforce-BlipForImageTextRetrieval-blip-itm-large-coco-new-LR-EPO.pt':{ # BEST BlipForImageTextRetrieval \"Salesforce/blip-itm-large-coco\"\n",
    "        'model_': 'BlipForImageTextRetrieval' , \n",
    "        'pretrained_model': \"Salesforce/blip-itm-large-coco\",\n",
    "        'processer': 'BLIPProcessDataset'\n",
    "    },\n",
    "    'blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-cross.pt':{ # BEST BlipModel \"Salesforce/blip-image-captioning-large\" CROSS\n",
    "        'model_': 'BlipModel' , \n",
    "        'pretrained_model': \"Salesforce/blip-image-captioning-large\",\n",
    "        'processer': 'BLIPProcessDataset',\n",
    "        'fusion': 'cross'\n",
    "    },\n",
    "    'blip_entire_model_Salesforce-BlipModel-blip2-inn-concat.pt':{ # BEST Blip2Model \"Salesforce/blip2-opt-2.7b\"\n",
    "        'model_': 'Blip2Model' , \n",
    "        'pretrained_model': \"Salesforce/blip2-opt-2.7b\",\n",
    "        'processer': 'BLIP2ProcessDataset'\n",
    "    },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat.pt':{\n",
    "    #     'model_': 'Blip2Model' , \n",
    "    #     'pretrained_model': \"Salesforce/blip2-flan-t5-xl\",\n",
    "    #     'processer': 'BLIP2ProcessDataset'\n",
    "    # },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5.pt':{\n",
    "    #     'model_': 'Blip2Model' , \n",
    "    #     'pretrained_model': \"Salesforce/blip2-flan-t5-xl\",\n",
    "    #     'processer': 'BLIP2ProcessDataset'\n",
    "    # },\n",
    "    'blip_entire_model_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5-LR-5e-3.pt':{\n",
    "        'model_': 'Blip2Model' , \n",
    "        'pretrained_model': \"Salesforce/blip2-flan-t5-xl\",\n",
    "        'processer': 'BLIP2ProcessDataset'\n",
    "    },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip2-inn-concat-epo30.pt':{\n",
    "    #     'model_': 'Blip2Model' , \n",
    "    #     'pretrained_model': \"Salesforce/blip2-opt-2.7b\",\n",
    "    #     'processer': 'BLIP2ProcessDataset'\n",
    "    # },\n",
    "    # 'dino_large_bge.pt':{\n",
    "    #     'model_': 'facebook/dinov2-large' , \n",
    "    #     'pretrained_img_model': 'facebook/dinov2-large',\n",
    "    #     'pretrained_txt_model': 'BAAI/bge-m3',\n",
    "    #     'processer': 'DinoProcessDataset',\n",
    "    #     'fusion': 'concat'\n",
    "    # },\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c83d73-01ba-4656-9b8f-e0d48fa83005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:02:15.680812Z",
     "iopub.status.busy": "2024-04-29T09:02:15.680312Z",
     "iopub.status.idle": "2024-04-29T09:05:41.868565Z",
     "shell.execute_reply": "2024-04-29T09:05:41.867393Z",
     "shell.execute_reply.started": "2024-04-29T09:02:15.680783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-LR-EPO.pt 43,393\n",
      "blip_entire_model_Salesforce-BlipForImageTextRetrieval-blip-itm-large-coco-new-LR-EPO.pt 1,488,642\n",
      "blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-cross.pt 43,393\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 14.98 GiB already allocated; 19.75 MiB free; 15.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m res_map\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_output/selected/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     pytorch_total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:,}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pytorch_total_params))\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_UntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/_utils.py:78\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_UntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 14.98 GiB already allocated; 19.75 MiB free; 15.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for fp in res_map.keys():\n",
    "    # print(model)\n",
    "    model = torch.load(f'model_output/selected/{fp}')\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(fp, '{:,}'.format(pytorch_total_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83fd86c-3320-4e63-8fee-8973cdc20bd2",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-29T09:47:25.033148Z",
     "iopub.status.busy": "2024-04-29T09:47:25.032683Z",
     "iopub.status.idle": "2024-04-29T09:49:29.842125Z",
     "shell.execute_reply": "2024-04-29T09:49:29.841141Z",
     "shell.execute_reply.started": "2024-04-29T09:47:25.033121Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blip_entire_model_Salesforce-BlipModel-blip2-inn-concat.pt 5,903,361\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.75 GiB total capacity; 22.41 GiB already allocated; 18.75 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblip_entire_model_Salesforce-BlipModel-blip2-inn-concat.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblip_entire_model_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5-LR-5e-3.pt\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_output/selected/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     pytorch_total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:,}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pytorch_total_params))\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_UntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/_utils.py:78\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_UntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.75 GiB total capacity; 22.41 GiB already allocated; 18.75 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "for fp in ['blip_entire_model_Salesforce-BlipModel-blip2-inn-concat.pt']:\n",
    "    # print(model)\n",
    "    model = torch.load(f'model_output/selected/{fp}')\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(fp, '{:,}'.format(pytorch_total_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "603fbdfa-8cb4-475c-a034-4480ede0f41e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T03:21:56.896022Z",
     "iopub.status.busy": "2024-04-30T03:21:56.895517Z",
     "iopub.status.idle": "2024-04-30T03:21:56.908922Z",
     "shell.execute_reply": "2024-04-30T03:21:56.908083Z",
     "shell.execute_reply.started": "2024-04-30T03:21:56.895991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blip_entire_model_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5-LR-5e-3.pt 46,674,945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "for fp in ['blip_entire_model_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5-LR-5e-3.pt']:\n",
    "    # print(model)\n",
    "    model = torch.load(f'model_output/selected/{fp}')\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(fp, '{:,}'.format(pytorch_total_params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87300941-7bff-4e2b-a388-5eb19a57bc8e",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ba86e7-9b17-4b68-98a3-066489439ad2",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T03:19:30.253412Z",
     "iopub.status.busy": "2024-04-30T03:19:30.252895Z",
     "iopub.status.idle": "2024-04-30T03:19:30.266918Z",
     "shell.execute_reply": "2024-04-30T03:19:30.266194Z",
     "shell.execute_reply.started": "2024-04-30T03:19:30.253382Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3942446592"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "3,942,446,592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11d6a23-ed29-4f5b-b925-f9749a49e81f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_map.0.weight 1441792\n",
      "image_map.0.bias 1024\n",
      "image_map.3.weight 1048576\n",
      "image_map.3.bias 1024\n",
      "image_map.6.weight 1048576\n",
      "image_map.6.bias 1024\n",
      "image_map.9.weight 1048576\n",
      "image_map.9.bias 1024\n",
      "image_map.12.weight 1048576\n",
      "image_map.12.bias 1024\n",
      "text_map.0.weight 32899072\n",
      "text_map.0.bias 1024\n",
      "text_map.3.weight 1048576\n",
      "text_map.3.bias 1024\n",
      "text_map.6.weight 1048576\n",
      "text_map.6.bias 1024\n",
      "text_map.9.weight 1048576\n",
      "text_map.9.bias 1024\n",
      "text_map.12.weight 1048576\n",
      "text_map.12.bias 1024\n",
      "qformer_map.0.weight 786432\n",
      "qformer_map.0.bias 1024\n",
      "pre_output.1.weight 3145728\n",
      "pre_output.1.bias 1024\n",
      "classifier.weight 1024\n",
      "classifier.bias 1\n"
     ]
    }
   ],
   "source": [
    "# blip_entire_model_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5-LR-5e-3.pt\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(name, p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678b27e5-934f-488c-8305-c531726747c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2491392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1441792+1024+1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db979abb-45bc-4957-ac4c-ee65bf1006ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435519512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "758*758*758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f482a873-17e8-4263-ba54-6bac06dbf1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBLIP(\n",
       "  (blip): Blip2Model(\n",
       "    (vision_model): Blip2VisionModel(\n",
       "      (embeddings): Blip2VisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (encoder): Blip2Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (12): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (13): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (14): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (15): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (16): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (17): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (18): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (19): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (20): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (21): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (22): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (23): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (24): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (25): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (26): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (27): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (28): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (29): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (30): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (31): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (32): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (33): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (34): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (35): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (36): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (37): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (38): Blip2EncoderLayer(\n",
       "            (self_attn): Blip2Attention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "              (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Blip2MLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "              (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (qformer): Blip2QFormerModel(\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (encoder): Blip2QFormerEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Blip2QFormerLayer(\n",
       "            (attention): Blip2QFormerAttention(\n",
       "              (attention): Blip2QFormerMultiHeadAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): Blip2QFormerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate_query): Blip2QFormerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): Blip2QFormerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_projection): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (language_model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 2048)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 2048)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (12): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (13): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (14): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (15): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (16): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (17): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (18): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (19): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (20): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (21): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (22): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (23): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 2048)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (12): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (13): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (14): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (15): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (16): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (17): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (18): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (19): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (20): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (21): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (22): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (23): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): GELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (image_map): Sequential(\n",
       "    (0): Linear(in_features=1408, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (10): Dropout(p=0.1, inplace=False)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (13): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (text_map): Sequential(\n",
       "    (0): Linear(in_features=32128, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (10): Dropout(p=0.1, inplace=False)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (13): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (qformer_map): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pre_output): Sequential(\n",
       "    (0): Dropout(p=0.4, inplace=False)\n",
       "    (1): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (cross_entropy_loss): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f78943-bf65-4340-901b-439526b11723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
