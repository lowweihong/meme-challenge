{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da40d644-7443-4e0d-9b9f-91b4b705ef45",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoProcessor, AutoTokenizer, Blip2Processor, Blip2Model, AutoTokenizer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "def base64str_to_PILobj(base64_string):\n",
    "    '''\n",
    "    Args\n",
    "    - base64_string (str): based64 encoded representing an image\n",
    "\n",
    "    Output\n",
    "    - PIL object (use .show() to display)\n",
    "    '''\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(image_data))\n",
    "    #img.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f30f867-346c-437d-bf27-60da128c3333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4eaddd3-5d18-4dcf-a62c-c1ab9c75234f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_result_df(data_loader, model, sample_n=None, type_='train'):\n",
    "    fina_res_dict_train = {\n",
    "        'pred': [],\n",
    "        'pred_score': [],\n",
    "        'idx': [],\n",
    "        'img': [],\n",
    "        'labels': [],\n",
    "        'type': []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "\n",
    "            output = model(batch, device=device)\n",
    "            predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "            # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "            fina_res_dict_train['pred'].extend(predicted.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict_train['pred_score'].extend(output.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict_train['idx'].extend(batch['idx_memes']) \n",
    "            fina_res_dict_train['labels'].extend(batch['labels'].detach().cpu().numpy().reshape(-1).tolist()) \n",
    "            fina_res_dict_train['img'].extend(batch['image'])\n",
    "            fina_res_dict_train['type'].extend([type_ for _ in range(len(batch['image']))])\n",
    "            \n",
    "            # break\n",
    "            if sample_n is not None and len(fina_res_dict_train['idx'])>=sample_n:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(fina_res_dict_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6eaaa9-5de8-4990-aa91-0036ce6f7fd3",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "res_map = {\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip-image-captioning-large-inn.pt':{\n",
    "    #     'model_':'BlipModel',\n",
    "    #     'pretrained_model': \"Salesforce/blip-image-captioning-large\",\n",
    "    #     'processer': 'BLIPProcessDataset'\n",
    "    # },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipForImageTextRetrieval-blip-itm-large-coco-new.pt':{\n",
    "    #     'model_': 'BlipForImageTextRetrieval',\n",
    "    #     'pretrained_model': 'Salesforce/blip-itm-large-coco',\n",
    "    #     'processer': 'BLIPProcessDataset'\n",
    "    # },\n",
    "    'blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-LR-EPO.pt':{ # BEST BlipModel \"Salesforce/blip-image-captioning-large\"\n",
    "        'model_': 'BlipModel', \n",
    "        'pretrained_model': \"Salesforce/blip-image-captioning-large\",\n",
    "        'processer': 'BLIPProcessDataset'\n",
    "    },\n",
    "    'blip_entire_model_Salesforce-BlipForImageTextRetrieval-blip-itm-large-coco-new-LR-EPO.pt':{ # BEST BlipForImageTextRetrieval \"Salesforce/blip-itm-large-coco\"\n",
    "        'model_': 'BlipForImageTextRetrieval' , \n",
    "        'pretrained_model': \"Salesforce/blip-itm-large-coco\",\n",
    "        'processer': 'BLIPProcessDataset'\n",
    "    },\n",
    "    # 'blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-cross.pt':{ # BEST BlipModel \"Salesforce/blip-image-captioning-large\" CROSS\n",
    "    #     'model_': 'BlipModel' , \n",
    "    #     'pretrained_model': \"Salesforce/blip-image-captioning-large\",\n",
    "    #     'processer': 'BLIPProcessDataset',\n",
    "    #     'fusion': 'cross'\n",
    "    # },\n",
    "    'blip_entire_model_Salesforce-BlipModel-blip2-inn-concat.pt':{ # BEST Blip2Model \"Salesforce/blip2-opt-2.7b\"\n",
    "        'model_': 'Blip2Model' , \n",
    "        'pretrained_model': \"Salesforce/blip2-opt-2.7b\",\n",
    "        'processer': 'BLIP2ProcessDataset'\n",
    "    },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat.pt':{\n",
    "    #     'model_': 'Blip2Model' , \n",
    "    #     'pretrained_model': \"Salesforce/blip2-flan-t5-xl\",\n",
    "    #     'processer': 'BLIP2ProcessDataset'\n",
    "    # },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5.pt':{\n",
    "    #     'model_': 'Blip2Model' , \n",
    "    #     'pretrained_model': \"Salesforce/blip2-flan-t5-xl\",\n",
    "    #     'processer': 'BLIP2ProcessDataset'\n",
    "    # },\n",
    "    'blip_entire_model_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5-LR-5e-3.pt':{\n",
    "        'model_': 'Blip2Model' , \n",
    "        'pretrained_model': \"Salesforce/blip2-flan-t5-xl\",\n",
    "        'processer': 'BLIP2ProcessDataset'\n",
    "    },\n",
    "    # 'blip_entire_model_kx_Salesforce-BlipModel-blip2-inn-concat-epo30.pt':{\n",
    "    #     'model_': 'Blip2Model' , \n",
    "    #     'pretrained_model': \"Salesforce/blip2-opt-2.7b\",\n",
    "    #     'processer': 'BLIP2ProcessDataset'\n",
    "    # },\n",
    "    # 'dino_large_bge.pt':{\n",
    "    #     'model_': 'facebook/dinov2-large' , \n",
    "    #     'pretrained_img_model': 'facebook/dinov2-large',\n",
    "    #     'pretrained_txt_model': 'BAAI/bge-m3',\n",
    "    #     'processer': 'DinoProcessDataset',\n",
    "    #     'fusion': 'concat'\n",
    "    # },\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6eb7ed4-7ca7-4b1b-a66f-8eb46a02e7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined = load_from_disk('./processed_data/combined_hateful_memes_dataset')\n",
    "\n",
    "train_data = combined['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51c4e15-7820-4047-8716-9c357e63eb75",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "# class DinoProcessDataset(Dataset):\n",
    "#     def __init__(self, dataset, pretrained_img_model, pretrained_txt_model, device='cuda'):\n",
    "#         self.image_size = 224\n",
    "#         self.dataset = dataset\n",
    "#         # self.processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "#         # Image processer\n",
    "#         self.processor = AutoImageProcessor.from_pretrained(pretrained_model, device=device)\n",
    "#         self.model = AutoModel.from_pretrained(pretrained_model)\n",
    "#         self.model.to(device)\n",
    "\n",
    "#         self.text_model = BGEM3FlagModel(pretrained_txt_model,  use_fp16=False, device=device)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.dataset[idx]\n",
    "#         device='cuda'\n",
    "       \n",
    "#         # pixel_values = self.processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\").resize((self.image_size, self.image_size)),\n",
    "#         #                                     return_tensors=\"pt\")['pixel_values']\n",
    "    \n",
    "#         # text_output = self.processor(text = item['text'],\n",
    "#         #                                   padding='max_length', \n",
    "#         #                                   return_tensors=\"pt\", \n",
    "#         #                                   truncation=True)\n",
    "#         # img = base64str_to_PILobj(item[\"image\"]).convert(\"RGB\").resize(self.image_size, self.image_size).to(device)\n",
    "#         inputs = self.processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\").resize((self.image_size, self.image_size)), \n",
    "#                                 return_tensors=\"pt\").to(device)\n",
    "#         pixel_out = self.model(**inputs)\n",
    "        \n",
    "#         pixel_values = pixel_out.last_hidden_state\n",
    "\n",
    "#         text_embeddings = self.text_model.encode(item['text'])['dense_vecs']#.to(device)\n",
    "\n",
    "#         label = torch.LongTensor([item['label']])\n",
    "\n",
    "#         return {\n",
    "#             'pixel_values': pixel_values,\n",
    "#             'text_output': text_embeddings,\n",
    "#             'labels': label,\n",
    "#             'idx_memes': item['id'],\n",
    "#             'image': item['image']\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd861d19-21df-4613-8b22-d972b7154689",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BLIPProcessDataset(Dataset):\n",
    "    def __init__(self, dataset, pretrained_model):\n",
    "        self.image_size = 224\n",
    "        self.dataset = dataset\n",
    "        self.processor = AutoProcessor.from_pretrained(pretrained_model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "       \n",
    "        pixel_values = self.processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\").resize((self.image_size, self.image_size)),\n",
    "                                            return_tensors=\"pt\")['pixel_values']\n",
    "    \n",
    "        text_output = self.processor(text=item['text'],\n",
    "                                     padding='max_length', \n",
    "                                     return_tensors=\"pt\", \n",
    "                                     truncation=True\n",
    "                                     )\n",
    "        # print(text_output.keys())\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        label = torch.LongTensor([item['label']])\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': text_output['input_ids'],\n",
    "            'attention_mask': text_output['attention_mask'],\n",
    "            'labels': label,\n",
    "            'idx_memes': item['id'],\n",
    "            'image': item['image']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d30f9b99-4e63-4dcc-8ab8-eaae675b156b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BLIP2ProcessDataset(Dataset):\n",
    "    def __init__(self, dataset, pretrained_model):\n",
    "        self.image_size = 518#224\n",
    "        self.dataset = dataset\n",
    "        self.processor = Blip2Processor.from_pretrained(pretrained_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "       \n",
    "        pixel_values = self.processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\").resize((self.image_size, self.image_size)),\n",
    "                                            return_tensors=\"pt\")['pixel_values']\n",
    "        \n",
    "        text_output = self.tokenizer(text=item['text'],\n",
    "                                     padding='max_length', \n",
    "                                     return_tensors=\"pt\", \n",
    "                                     max_length=512,\n",
    "                                     truncation=True\n",
    "                                     )\n",
    "        # print(text_output.keys())\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        label = torch.LongTensor([item['label']])\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': text_output['input_ids'],\n",
    "            'attention_mask': text_output['attention_mask'],\n",
    "            'labels': label,\n",
    "            'idx_memes': item['id'],\n",
    "            'image': item['image'],\n",
    "            'pad_token_id': self.tokenizer.pad_token_id,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2e8566-5e66-4551-80c4-83e60e8cae97",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-LR-EPO.pt\n",
      "dev_unseen 0.6379558823529412 0.6407407407407407\n",
      "dev_seen 0.6485893968731498 0.592\n",
      "test_unseen 0.678768 0.661\n",
      "test_seen 0.6574309723889555 0.603\n",
      "==========\n",
      "blip_entire_model_Salesforce-BlipForImageTextRetrieval-blip-itm-large-coco-new-LR-EPO.pt\n",
      "dev_unseen 0.6908676470588235 0.6462962962962963\n",
      "dev_seen 0.699876782256645 0.546\n",
      "test_unseen 0.6792970666666667 0.652\n",
      "test_seen 0.6891156462585034 0.565\n",
      "==========\n",
      "blip_entire_model_Salesforce-BlipModel-blip2-inn-concat.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 31.74 GiB total capacity; 19.32 GiB already allocated; 1.55 GiB free; 20.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     35\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev_unseen\u001b[39m\u001b[38;5;124m'\u001b[39m: dev_unseen_loader,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev_seen\u001b[39m\u001b[38;5;124m'\u001b[39m: dev_seen_loader,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_unseen\u001b[39m\u001b[38;5;124m'\u001b[39m: test_unseen_loader,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_seen\u001b[39m\u001b[38;5;124m'\u001b[39m: test_seen_loader,\n\u001b[1;32m     40\u001b[0m }\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m type_, type_loader \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 43\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mget_result_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(type_, roc_auc_score(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_score\u001b[39m\u001b[38;5;124m'\u001b[39m]), accuracy_score(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mget_result_df\u001b[0;34m(data_loader, model, sample_n, type_)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m---> 17\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor((output \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/workspace/weihong/project/models/blip2_feasExtract.py:105\u001b[0m, in \u001b[0;36mCustomBLIP.forward\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblip\u001b[38;5;241m.\u001b[39mget_image_features(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    101\u001b[0m                                             return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mpooler_output \u001b[38;5;66;03m#[bs, 1408]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# **batch)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m text_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:,:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;66;03m#[0] #[bs, 512, 50272]\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# **batch)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m qformer_feas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblip\u001b[38;5;241m.\u001b[39mget_qformer_features(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    113\u001b[0m                                             \u001b[38;5;66;03m# input_ids=input_ids.squeeze(1), \u001b[39;00m\n\u001b[1;32m    114\u001b[0m                                             \u001b[38;5;66;03m# attention_mask=attention_mask.squeeze(1),\u001b[39;00m\n\u001b[1;32m    115\u001b[0m                                            return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mpooler_output \u001b[38;5;66;03m#[bs, 768]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/transformers/models/blip_2/modeling_blip_2.py:1271\u001b[0m, in \u001b[0;36mBlip2Model.get_text_features\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1268\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_decoder_only_language_model:\n\u001b[0;32m-> 1271\u001b[0m     text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()(input_ids)\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/transformers/models/opt/modeling_opt.py:879\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 879\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    893\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/transformers/models/opt/modeling_opt.py:645\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    635\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    636\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    637\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m         use_cache,\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 645\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/transformers/models/opt/modeling_opt.py:299\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    296\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    307\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/translation_dev/lib/python3.8/site-packages/transformers/models/opt/modeling_opt.py:192\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;241m1\u001b[39m, tgt_len, src_len):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention mask should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n\u001b[0;32m--> 192\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    193\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\n\u001b[1;32m    194\u001b[0m     attn_weights, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    195\u001b[0m )\n\u001b[1;32m    196\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 31.74 GiB total capacity; 19.32 GiB already allocated; 1.55 GiB free; 20.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "device='cuda'\n",
    "\n",
    "done = []\n",
    "for fp, map_dict in res_map.items():\n",
    "    \n",
    "    print('==========')\n",
    "    print(fp)\n",
    "    if 'flan' in fp:\n",
    "        import blip2_feasExtract_flan as blip2_feasExtract\n",
    "        models.blip2_feasExtract = blip2_feasExtract\n",
    "    model = torch.load(f'model_output/selected/{fp}')\n",
    "    \n",
    "    # Generate process DataSet\n",
    "    processer = eval(map_dict['processer'])\n",
    "    train_dataset = processer(train_data, map_dict['pretrained_model'])\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    dev_seen_data = combined['dev_seen']\n",
    "    dev_seen_dataset = processer(dev_seen_data, map_dict['pretrained_model'])\n",
    "    dev_seen_loader = DataLoader(dev_seen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    dev_unseen_data = combined['dev_unseen']\n",
    "    dev_unseen_dataset = processer(dev_unseen_data, map_dict['pretrained_model'])\n",
    "    dev_unseen_loader = DataLoader(dev_unseen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    test_seen_data = combined['test_seen']\n",
    "    test_seen_dataset = processer(test_seen_data, map_dict['pretrained_model'])\n",
    "    test_seen_loader = DataLoader(test_seen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    test_unseen_data = combined['test_unseen']\n",
    "    test_unseen_dataset = processer(test_unseen_data, map_dict['pretrained_model'])\n",
    "    test_unseen_loader = DataLoader(test_unseen_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    data_dict = {\n",
    "        'dev_unseen': dev_unseen_loader,\n",
    "        'dev_seen': dev_seen_loader,\n",
    "        'test_unseen': test_unseen_loader,\n",
    "        'test_seen': test_seen_loader,\n",
    "    }\n",
    "    \n",
    "    for type_, type_loader in data_dict.items():\n",
    "        df = get_result_df(type_loader, model, type_=type_)\n",
    "\n",
    "        print(type_, roc_auc_score(df['labels'], df['pred_score']), accuracy_score(df['labels'], df['pred']))\n",
    "    \n",
    "    del model\n",
    "    # print('==========')\n",
    "    # break\n",
    "    done.append(fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73724f59-b456-48a0-acbd-f2ab8ae63ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blip_entire_model_Salesforce-BlipModel-blip-image-captioning-large-inn-LR-EPO.pt',\n",
       " 'blip_entire_model_Salesforce-BlipForImageTextRetrieval-blip-itm-large-coco-new-LR-EPO.pt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c208378a-38ce-415a-94fb-7613b99b9151",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "blip_entire_model_Salesforce-BlipModel-blip2-inn-concat.pt\n",
      "dev_unseen 0.7262352941176472 0.6907407407407408\n",
      "dev_seen 0.7405226352594774 0.64\n",
      "test_unseen 0.7454357333333334 0.6975\n",
      "test_seen 0.7449099639855942 0.652\n",
      "==========\n",
      "blip_entire_model_Salesforce-BlipModel-blip2-flan-t5-xlinn-concat-layer5-LR-5e-3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefc531616174667ba24ccfbef0ce7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec8cf4a6bc847dbb0196ca1027f6cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bda33fb31fd46d6ae51df306251749c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88010ce9648b433e82077b8da6ad8763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f83a3b5c3bc4973abd30e7ccd0f6f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_unseen 0.7332205882352941 0.7092592592592593\n",
      "dev_seen 0.7509881422924901 0.68\n",
      "test_unseen 0.764216 0.7155\n",
      "test_seen 0.7682553021208484 0.688\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "device='cuda'\n",
    "\n",
    "for fp, map_dict in res_map.items():\n",
    "    if fp in done:\n",
    "        continue\n",
    "    print('==========')\n",
    "    print(fp)\n",
    "    model = torch.load(f'model_output/selected/{fp}')\n",
    "    \n",
    "    # Generate process DataSet\n",
    "    processer = eval(map_dict['processer'])\n",
    "    train_dataset = processer(train_data, map_dict['pretrained_model'])\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    dev_seen_data = combined['dev_seen']\n",
    "    dev_seen_dataset = processer(dev_seen_data, map_dict['pretrained_model'])\n",
    "    dev_seen_loader = DataLoader(dev_seen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    dev_unseen_data = combined['dev_unseen']\n",
    "    dev_unseen_dataset = processer(dev_unseen_data, map_dict['pretrained_model'])\n",
    "    dev_unseen_loader = DataLoader(dev_unseen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    test_seen_data = combined['test_seen']\n",
    "    test_seen_dataset = processer(test_seen_data, map_dict['pretrained_model'])\n",
    "    test_seen_loader = DataLoader(test_seen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    test_unseen_data = combined['test_unseen']\n",
    "    test_unseen_dataset = processer(test_unseen_data, map_dict['pretrained_model'])\n",
    "    test_unseen_loader = DataLoader(test_unseen_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    data_dict = {\n",
    "        'dev_unseen': dev_unseen_loader,\n",
    "        'dev_seen': dev_seen_loader,\n",
    "        'test_unseen': test_unseen_loader,\n",
    "        'test_seen': test_seen_loader,\n",
    "    }\n",
    "    \n",
    "    for type_, type_loader in data_dict.items():\n",
    "        df = get_result_df(type_loader, model, type_=type_)\n",
    "\n",
    "        print(type_, roc_auc_score(df['labels'], df['pred_score']), accuracy_score(df['labels'], df['pred']))\n",
    "    \n",
    "    del model\n",
    "    # print('==========')\n",
    "    # break\n",
    "    done.append(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63465fe-dc37-4c4d-b912-a19a4352677a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdaeca-8fd3-4892-bacd-e9b685baae1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e5529b6-facd-4b71-bcfa-2918800d8015",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-29T03:00:09.589450Z",
     "iopub.status.busy": "2024-04-29T03:00:09.588904Z",
     "iopub.status.idle": "2024-04-29T03:00:09.801440Z",
     "shell.execute_reply": "2024-04-29T03:00:09.800183Z",
     "shell.execute_reply.started": "2024-04-29T03:00:09.589418Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26449281024\n",
      "23763481088\n"
     ]
    }
   ],
   "source": [
    "# # To free up CUDA mem after every load\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# print(torch.cuda.memory_reserved(0))\n",
    "# print(torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69ce1c-137f-4998-a76d-94a162f7008e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6380daf496d3ca9e469896d457f38fdc052da6cb77af8013b47946477efd6c5a"
  },
  "kernelspec": {
   "display_name": "translation_dev",
   "language": "python",
   "name": "translation_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
