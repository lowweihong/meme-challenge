{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5589d4f9-12c1-4b7d-89fd-f37153b01502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_from_disk\n",
    "from transformers import CLIPTokenizer, CLIPProcessor, AutoTokenizer\n",
    "from transformers import AutoProcessor, FlavaModel\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8184de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.image_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "        self.text_processor = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # encoding = self.processor(images=base64str_to_PILobj(item[\"image\"]), text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # # remove batch dimension\n",
    "        # encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        # # import pdb; pdb.set_trace()\n",
    "        # encoding['label'] = item['label']\n",
    "        # encoding['image'] = item[\"image\"]\n",
    "        # return encoding\n",
    "        pixel_values = self.image_processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\"),\n",
    "                                            return_tensors=\"pt\")['pixel_values']\n",
    "        # caption_output = self.text_processor(item[\"caption\"], \n",
    "        #                                      padding=True,\n",
    "        #                                      return_tensors=\"pt\",\n",
    "        #                                      truncation=True)\n",
    "        text_output = self.text_processor(item['text'],\n",
    "                                          padding='max_length', \n",
    "                                          return_tensors=\"pt\", \n",
    "                                          truncation=True)\n",
    "        # pdb.set_trace()\n",
    "        # print(idx, pixel_values.shape)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        label = torch.LongTensor([item['label']])\n",
    "        # import pdb; pdb.set_trace()\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': text_output['input_ids'],\n",
    "            'attention_mask': text_output['attention_mask'],\n",
    "            'labels': label,\n",
    "            # 'input_ids_caption': caption_output['input_ids'],\n",
    "            # 'attention_mask_caption': caption_output['attention_mask_caption'],\n",
    "            'idx_memes': item['id'],\n",
    "            'image': item['image']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 map_dim=32,\n",
    "                 dropout_lst=[0.1, 0.4, 0.2],\n",
    "                 pretrained_model='openai/clip-vit-large-patch14',\n",
    "                 freeze_image_encoder=True,\n",
    "                 freeze_text_encoder=True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.map_dim = map_dim\n",
    "        self.dropout_lst = dropout_lst\n",
    "        self.num_mapping_layers = 1\n",
    "        self.head = 'concat'\n",
    "\n",
    "        self.clip = CLIPModel.from_pretrained(pretrained_model)\n",
    "        self.image_encoder = copy.deepcopy(self.clip.vision_model)\n",
    "        self.text_encoder = copy.deepcopy(self.clip.text_model)\n",
    "\n",
    "        # Not using pretrained map\n",
    "        image_map_layers = [nn.Linear(self.image_encoder.config.hidden_size, self.map_dim),\n",
    "                            nn.Dropout(p=self.dropout_lst[0])]\n",
    "        text_map_layers = [nn.Linear(self.text_encoder.config.hidden_size, self.map_dim),\n",
    "                           nn.Dropout(p=self.dropout_lst[0])]\n",
    "        for _ in range(1, self.num_mapping_layers):\n",
    "            image_map_layers.extend([nn.ReLU(), \n",
    "                                     nn.Linear(self.map_dim, self.map_dim), \n",
    "                                     nn.Dropout(p=self.dropout_lst[0])])\n",
    "            text_map_layers.extend([nn.ReLU(), \n",
    "                                    nn.Linear(self.map_dim, self.map_dim), \n",
    "                                    nn.Dropout(p=self.dropout_lst[0])])\n",
    "\n",
    "        self.image_map = nn.Sequential(*image_map_layers)\n",
    "        self.text_map = nn.Sequential(*text_map_layers)\n",
    "\n",
    "        pre_output_input_dim = self.map_dim*2\n",
    "\n",
    "        pre_output_layers = [nn.Dropout(p=self.dropout_lst[1])]\n",
    "        pre_output_layers.extend([nn.Linear(pre_output_input_dim, self.map_dim),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=self.dropout_lst[2])])\n",
    "\n",
    "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
    "        self.output = nn.Linear(self.map_dim, 1)\n",
    "\n",
    "        self.cross_entropy_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        if freeze_image_encoder:\n",
    "            for _, p in self.image_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        if freeze_text_encoder:\n",
    "            for _, p in self.text_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "        del self.clip\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        image_features = self.image_encoder(pixel_values=pixel_values.squeeze(1)).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        text_features = self.text_encoder(input_ids=input_ids.squeeze(1),\n",
    "                                          attention_mask=attention_mask.squeeze(1)).pooler_output\n",
    "        \n",
    "        text_features = self.text_map(text_features)\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "        if self.head == 'concat':\n",
    "            features = torch.cat([image_features, text_features], dim=1)\n",
    "        elif self.head == 'cross':\n",
    "            features = torch.bmm(image_features.unsqueeze(2), text_features.unsqueeze(1)) # [16, d, d]\n",
    "            features = features.reshape(features.shape[0], -1)  # [16, d*d]\n",
    "\n",
    "        features = self.pre_output(features)\n",
    "        logits = self.output(features)\n",
    "        preds = torch.sigmoid(logits)\n",
    "        # preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30d238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (image_encoder): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_map): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (text_map): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pre_output): Sequential(\n",
       "    (0): Dropout(p=0.4, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (cross_entropy_loss): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('clip_entire_model_added_sigmoid_gradient_clip.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba55d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.image_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "        self.text_processor = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # encoding = self.processor(images=base64str_to_PILobj(item[\"image\"]), text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # # remove batch dimension\n",
    "        # encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        # # import pdb; pdb.set_trace()\n",
    "        # encoding['label'] = item['label']\n",
    "        # encoding['image'] = item[\"image\"]\n",
    "        # return encoding\n",
    "        pixel_values = self.image_processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\"),\n",
    "                                            return_tensors=\"pt\")['pixel_values']\n",
    "        # caption_output = self.text_processor(item[\"caption\"], \n",
    "        #                                      padding=True,\n",
    "        #                                      return_tensors=\"pt\",\n",
    "        #                                      truncation=True)\n",
    "        text_output = self.text_processor(item['text'],\n",
    "                                          padding='max_length', \n",
    "                                          return_tensors=\"pt\", \n",
    "                                          truncation=True)\n",
    "        # pdb.set_trace()\n",
    "        # print(idx, pixel_values.shape)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        label = torch.LongTensor([item['label']])\n",
    "        # import pdb; pdb.set_trace()\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': text_output['input_ids'],\n",
    "            'attention_mask': text_output['attention_mask'],\n",
    "            'labels': label,\n",
    "            # 'input_ids_caption': caption_output['input_ids'],\n",
    "            # 'attention_mask_caption': caption_output['attention_mask_caption'],\n",
    "            'idx_memes': item['id'],\n",
    "            'image': item['image']\n",
    "        }\n",
    "\n",
    "def base64str_to_PILobj(base64_string):\n",
    "    '''\n",
    "    Args\n",
    "    - base64_string (str): based64 encoded representing an image\n",
    "\n",
    "    Output\n",
    "    - PIL object (use .show() to display)\n",
    "    '''\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(image_data))\n",
    "    #img.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c5244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "combined = load_from_disk('./processed_data/combined_hateful_memes_dataset')\n",
    "# train_data = combined['train']\n",
    "# print('processing image...')\n",
    "# train_dataset = ImageCaptioningDataset(train_data)\n",
    "# train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "dev_seen_data = combined['dev_unseen']\n",
    "dev_seen_dataset = ImageCaptioningDataset(dev_seen_data)\n",
    "dev_seen_loader = DataLoader(dev_seen_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c29380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/32 correct for this batch. total corrected by far=21/32\n",
      "18/32 correct for this batch. total corrected by far=39/64\n",
      "23/32 correct for this batch. total corrected by far=62/96\n",
      "23/32 correct for this batch. total corrected by far=85/128\n",
      "19/32 correct for this batch. total corrected by far=104/160\n",
      "22/32 correct for this batch. total corrected by far=126/192\n",
      "17/32 correct for this batch. total corrected by far=143/224\n",
      "27/32 correct for this batch. total corrected by far=170/256\n",
      "20/32 correct for this batch. total corrected by far=190/288\n",
      "27/32 correct for this batch. total corrected by far=217/320\n",
      "18/32 correct for this batch. total corrected by far=235/352\n",
      "19/32 correct for this batch. total corrected by far=254/384\n",
      "18/32 correct for this batch. total corrected by far=272/416\n",
      "25/32 correct for this batch. total corrected by far=297/448\n",
      "21/32 correct for this batch. total corrected by far=318/480\n",
      "24/32 correct for this batch. total corrected by far=342/512\n",
      "23/28 correct for this batch. total corrected by far=365/540\n",
      "0.6759259259259259\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model.eval()\n",
    "correct_normal = 0\n",
    "total = 0\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    for batch in dev_seen_loader:\n",
    "        # print(len(batch))\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # token_type_ids  = batch['token_type_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(batch)\n",
    "        predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "        # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        this_batch_corrected = (predicted==labels.reshape(-1,1)).sum().item()\n",
    "        correct_normal += this_batch_corrected\n",
    "        print(f'{this_batch_corrected}/{labels.size(0)} correct for this batch. total corrected by far={correct_normal}/{total}')\n",
    "        # correct_normal += (predicted == labels).sum().item()\n",
    "        # break\n",
    "\n",
    "        \n",
    "accuracy = correct_normal/total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d85dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395aff9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "021424e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing image...\n"
     ]
    }
   ],
   "source": [
    "train_data = combined['train']\n",
    "print('processing image...')\n",
    "train_dataset = ImageCaptioningDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdb7610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/32 correct for this batch. total corrected by far=29/32\n",
      "27/32 correct for this batch. total corrected by far=56/64\n",
      "29/32 correct for this batch. total corrected by far=85/96\n",
      "27/32 correct for this batch. total corrected by far=112/128\n",
      "24/32 correct for this batch. total corrected by far=136/160\n",
      "26/32 correct for this batch. total corrected by far=162/192\n",
      "29/32 correct for this batch. total corrected by far=191/224\n",
      "29/32 correct for this batch. total corrected by far=220/256\n",
      "25/32 correct for this batch. total corrected by far=245/288\n",
      "29/32 correct for this batch. total corrected by far=274/320\n",
      "30/32 correct for this batch. total corrected by far=304/352\n",
      "28/32 correct for this batch. total corrected by far=332/384\n",
      "25/32 correct for this batch. total corrected by far=357/416\n",
      "28/32 correct for this batch. total corrected by far=385/448\n",
      "26/32 correct for this batch. total corrected by far=411/480\n",
      "29/32 correct for this batch. total corrected by far=440/512\n",
      "27/32 correct for this batch. total corrected by far=467/544\n",
      "30/32 correct for this batch. total corrected by far=497/576\n",
      "28/32 correct for this batch. total corrected by far=525/608\n",
      "27/32 correct for this batch. total corrected by far=552/640\n",
      "27/32 correct for this batch. total corrected by far=579/672\n",
      "27/32 correct for this batch. total corrected by far=606/704\n",
      "29/32 correct for this batch. total corrected by far=635/736\n",
      "25/32 correct for this batch. total corrected by far=660/768\n",
      "31/32 correct for this batch. total corrected by far=691/800\n",
      "29/32 correct for this batch. total corrected by far=720/832\n",
      "27/32 correct for this batch. total corrected by far=747/864\n",
      "25/32 correct for this batch. total corrected by far=772/896\n",
      "29/32 correct for this batch. total corrected by far=801/928\n",
      "24/32 correct for this batch. total corrected by far=825/960\n",
      "24/32 correct for this batch. total corrected by far=849/992\n",
      "28/32 correct for this batch. total corrected by far=877/1024\n",
      "30/32 correct for this batch. total corrected by far=907/1056\n",
      "25/32 correct for this batch. total corrected by far=932/1088\n",
      "27/32 correct for this batch. total corrected by far=959/1120\n",
      "25/32 correct for this batch. total corrected by far=984/1152\n",
      "23/32 correct for this batch. total corrected by far=1007/1184\n",
      "25/32 correct for this batch. total corrected by far=1032/1216\n",
      "24/32 correct for this batch. total corrected by far=1056/1248\n",
      "30/32 correct for this batch. total corrected by far=1086/1280\n",
      "28/32 correct for this batch. total corrected by far=1114/1312\n",
      "26/32 correct for this batch. total corrected by far=1140/1344\n",
      "28/32 correct for this batch. total corrected by far=1168/1376\n",
      "25/32 correct for this batch. total corrected by far=1193/1408\n",
      "27/32 correct for this batch. total corrected by far=1220/1440\n",
      "24/32 correct for this batch. total corrected by far=1244/1472\n",
      "27/32 correct for this batch. total corrected by far=1271/1504\n",
      "25/32 correct for this batch. total corrected by far=1296/1536\n",
      "25/32 correct for this batch. total corrected by far=1321/1568\n",
      "25/32 correct for this batch. total corrected by far=1346/1600\n",
      "32/32 correct for this batch. total corrected by far=1378/1632\n",
      "26/32 correct for this batch. total corrected by far=1404/1664\n",
      "28/32 correct for this batch. total corrected by far=1432/1696\n",
      "26/32 correct for this batch. total corrected by far=1458/1728\n",
      "30/32 correct for this batch. total corrected by far=1488/1760\n",
      "25/32 correct for this batch. total corrected by far=1513/1792\n",
      "28/32 correct for this batch. total corrected by far=1541/1824\n",
      "29/32 correct for this batch. total corrected by far=1570/1856\n",
      "30/32 correct for this batch. total corrected by far=1600/1888\n",
      "28/32 correct for this batch. total corrected by far=1628/1920\n",
      "29/32 correct for this batch. total corrected by far=1657/1952\n",
      "28/32 correct for this batch. total corrected by far=1685/1984\n",
      "28/32 correct for this batch. total corrected by far=1713/2016\n",
      "30/32 correct for this batch. total corrected by far=1743/2048\n",
      "23/32 correct for this batch. total corrected by far=1766/2080\n",
      "24/32 correct for this batch. total corrected by far=1790/2112\n",
      "27/32 correct for this batch. total corrected by far=1817/2144\n",
      "24/32 correct for this batch. total corrected by far=1841/2176\n",
      "26/32 correct for this batch. total corrected by far=1867/2208\n",
      "26/32 correct for this batch. total corrected by far=1893/2240\n",
      "30/32 correct for this batch. total corrected by far=1923/2272\n",
      "27/32 correct for this batch. total corrected by far=1950/2304\n",
      "29/32 correct for this batch. total corrected by far=1979/2336\n",
      "24/32 correct for this batch. total corrected by far=2003/2368\n",
      "24/32 correct for this batch. total corrected by far=2027/2400\n",
      "25/32 correct for this batch. total corrected by far=2052/2432\n",
      "20/32 correct for this batch. total corrected by far=2072/2464\n",
      "27/32 correct for this batch. total corrected by far=2099/2496\n",
      "23/32 correct for this batch. total corrected by far=2122/2528\n",
      "28/32 correct for this batch. total corrected by far=2150/2560\n",
      "24/32 correct for this batch. total corrected by far=2174/2592\n",
      "27/32 correct for this batch. total corrected by far=2201/2624\n",
      "26/32 correct for this batch. total corrected by far=2227/2656\n",
      "26/32 correct for this batch. total corrected by far=2253/2688\n",
      "27/32 correct for this batch. total corrected by far=2280/2720\n",
      "25/32 correct for this batch. total corrected by far=2305/2752\n",
      "24/32 correct for this batch. total corrected by far=2329/2784\n",
      "26/32 correct for this batch. total corrected by far=2355/2816\n",
      "24/32 correct for this batch. total corrected by far=2379/2848\n",
      "28/32 correct for this batch. total corrected by far=2407/2880\n",
      "24/32 correct for this batch. total corrected by far=2431/2912\n",
      "28/32 correct for this batch. total corrected by far=2459/2944\n",
      "29/32 correct for this batch. total corrected by far=2488/2976\n",
      "27/32 correct for this batch. total corrected by far=2515/3008\n",
      "27/32 correct for this batch. total corrected by far=2542/3040\n",
      "30/32 correct for this batch. total corrected by far=2572/3072\n",
      "28/32 correct for this batch. total corrected by far=2600/3104\n",
      "27/32 correct for this batch. total corrected by far=2627/3136\n",
      "25/32 correct for this batch. total corrected by far=2652/3168\n",
      "25/32 correct for this batch. total corrected by far=2677/3200\n",
      "29/32 correct for this batch. total corrected by far=2706/3232\n",
      "26/32 correct for this batch. total corrected by far=2732/3264\n",
      "29/32 correct for this batch. total corrected by far=2761/3296\n",
      "28/32 correct for this batch. total corrected by far=2789/3328\n",
      "24/32 correct for this batch. total corrected by far=2813/3360\n",
      "26/32 correct for this batch. total corrected by far=2839/3392\n",
      "27/32 correct for this batch. total corrected by far=2866/3424\n",
      "25/32 correct for this batch. total corrected by far=2891/3456\n",
      "24/32 correct for this batch. total corrected by far=2915/3488\n",
      "28/32 correct for this batch. total corrected by far=2943/3520\n",
      "26/32 correct for this batch. total corrected by far=2969/3552\n",
      "25/32 correct for this batch. total corrected by far=2994/3584\n",
      "26/32 correct for this batch. total corrected by far=3020/3616\n",
      "25/32 correct for this batch. total corrected by far=3045/3648\n",
      "27/32 correct for this batch. total corrected by far=3072/3680\n",
      "30/32 correct for this batch. total corrected by far=3102/3712\n",
      "28/32 correct for this batch. total corrected by far=3130/3744\n",
      "27/32 correct for this batch. total corrected by far=3157/3776\n",
      "29/32 correct for this batch. total corrected by far=3186/3808\n",
      "27/32 correct for this batch. total corrected by far=3213/3840\n",
      "27/32 correct for this batch. total corrected by far=3240/3872\n",
      "30/32 correct for this batch. total corrected by far=3270/3904\n",
      "29/32 correct for this batch. total corrected by far=3299/3936\n",
      "26/32 correct for this batch. total corrected by far=3325/3968\n",
      "26/32 correct for this batch. total corrected by far=3351/4000\n",
      "27/32 correct for this batch. total corrected by far=3378/4032\n",
      "22/32 correct for this batch. total corrected by far=3400/4064\n",
      "26/32 correct for this batch. total corrected by far=3426/4096\n",
      "26/32 correct for this batch. total corrected by far=3452/4128\n",
      "24/32 correct for this batch. total corrected by far=3476/4160\n",
      "30/32 correct for this batch. total corrected by far=3506/4192\n",
      "26/32 correct for this batch. total corrected by far=3532/4224\n",
      "30/32 correct for this batch. total corrected by far=3562/4256\n",
      "22/32 correct for this batch. total corrected by far=3584/4288\n",
      "26/32 correct for this batch. total corrected by far=3610/4320\n",
      "28/32 correct for this batch. total corrected by far=3638/4352\n",
      "26/32 correct for this batch. total corrected by far=3664/4384\n",
      "29/32 correct for this batch. total corrected by far=3693/4416\n",
      "30/32 correct for this batch. total corrected by far=3723/4448\n",
      "29/32 correct for this batch. total corrected by far=3752/4480\n",
      "27/32 correct for this batch. total corrected by far=3779/4512\n",
      "27/32 correct for this batch. total corrected by far=3806/4544\n",
      "29/32 correct for this batch. total corrected by far=3835/4576\n",
      "31/32 correct for this batch. total corrected by far=3866/4608\n",
      "28/32 correct for this batch. total corrected by far=3894/4640\n",
      "26/32 correct for this batch. total corrected by far=3920/4672\n",
      "24/32 correct for this batch. total corrected by far=3944/4704\n",
      "26/32 correct for this batch. total corrected by far=3970/4736\n",
      "26/32 correct for this batch. total corrected by far=3996/4768\n",
      "28/32 correct for this batch. total corrected by far=4024/4800\n",
      "26/32 correct for this batch. total corrected by far=4050/4832\n",
      "26/32 correct for this batch. total corrected by far=4076/4864\n",
      "28/32 correct for this batch. total corrected by far=4104/4896\n",
      "24/32 correct for this batch. total corrected by far=4128/4928\n",
      "27/32 correct for this batch. total corrected by far=4155/4960\n",
      "28/32 correct for this batch. total corrected by far=4183/4992\n",
      "27/32 correct for this batch. total corrected by far=4210/5024\n",
      "27/32 correct for this batch. total corrected by far=4237/5056\n",
      "28/32 correct for this batch. total corrected by far=4265/5088\n",
      "28/32 correct for this batch. total corrected by far=4293/5120\n",
      "31/32 correct for this batch. total corrected by far=4324/5152\n",
      "26/32 correct for this batch. total corrected by far=4350/5184\n",
      "30/32 correct for this batch. total corrected by far=4380/5216\n",
      "31/32 correct for this batch. total corrected by far=4411/5248\n",
      "25/32 correct for this batch. total corrected by far=4436/5280\n",
      "27/32 correct for this batch. total corrected by far=4463/5312\n",
      "24/32 correct for this batch. total corrected by far=4487/5344\n",
      "29/32 correct for this batch. total corrected by far=4516/5376\n",
      "28/32 correct for this batch. total corrected by far=4544/5408\n",
      "24/32 correct for this batch. total corrected by far=4568/5440\n",
      "30/32 correct for this batch. total corrected by far=4598/5472\n",
      "29/32 correct for this batch. total corrected by far=4627/5504\n",
      "30/32 correct for this batch. total corrected by far=4657/5536\n",
      "27/32 correct for this batch. total corrected by far=4684/5568\n",
      "28/32 correct for this batch. total corrected by far=4712/5600\n",
      "28/32 correct for this batch. total corrected by far=4740/5632\n",
      "29/32 correct for this batch. total corrected by far=4769/5664\n",
      "27/32 correct for this batch. total corrected by far=4796/5696\n",
      "27/32 correct for this batch. total corrected by far=4823/5728\n",
      "24/32 correct for this batch. total corrected by far=4847/5760\n",
      "25/32 correct for this batch. total corrected by far=4872/5792\n",
      "23/32 correct for this batch. total corrected by far=4895/5824\n",
      "29/32 correct for this batch. total corrected by far=4924/5856\n",
      "24/32 correct for this batch. total corrected by far=4948/5888\n",
      "26/32 correct for this batch. total corrected by far=4974/5920\n",
      "27/32 correct for this batch. total corrected by far=5001/5952\n",
      "31/32 correct for this batch. total corrected by far=5032/5984\n",
      "22/32 correct for this batch. total corrected by far=5054/6016\n",
      "27/32 correct for this batch. total corrected by far=5081/6048\n",
      "27/32 correct for this batch. total corrected by far=5108/6080\n",
      "31/32 correct for this batch. total corrected by far=5139/6112\n",
      "25/32 correct for this batch. total corrected by far=5164/6144\n",
      "24/32 correct for this batch. total corrected by far=5188/6176\n",
      "26/32 correct for this batch. total corrected by far=5214/6208\n",
      "30/32 correct for this batch. total corrected by far=5244/6240\n",
      "25/32 correct for this batch. total corrected by far=5269/6272\n",
      "27/32 correct for this batch. total corrected by far=5296/6304\n",
      "27/32 correct for this batch. total corrected by far=5323/6336\n",
      "28/32 correct for this batch. total corrected by far=5351/6368\n",
      "25/32 correct for this batch. total corrected by far=5376/6400\n",
      "28/32 correct for this batch. total corrected by far=5404/6432\n",
      "25/32 correct for this batch. total corrected by far=5429/6464\n",
      "29/32 correct for this batch. total corrected by far=5458/6496\n",
      "24/32 correct for this batch. total corrected by far=5482/6528\n",
      "24/32 correct for this batch. total corrected by far=5506/6560\n",
      "26/32 correct for this batch. total corrected by far=5532/6592\n",
      "30/32 correct for this batch. total corrected by far=5562/6624\n",
      "26/32 correct for this batch. total corrected by far=5588/6656\n",
      "29/32 correct for this batch. total corrected by far=5617/6688\n",
      "25/32 correct for this batch. total corrected by far=5642/6720\n",
      "23/32 correct for this batch. total corrected by far=5665/6752\n",
      "24/32 correct for this batch. total corrected by far=5689/6784\n",
      "30/32 correct for this batch. total corrected by far=5719/6816\n",
      "25/32 correct for this batch. total corrected by far=5744/6848\n",
      "23/32 correct for this batch. total corrected by far=5767/6880\n",
      "28/32 correct for this batch. total corrected by far=5795/6912\n",
      "26/32 correct for this batch. total corrected by far=5821/6944\n",
      "29/32 correct for this batch. total corrected by far=5850/6976\n",
      "26/32 correct for this batch. total corrected by far=5876/7008\n",
      "26/32 correct for this batch. total corrected by far=5902/7040\n",
      "20/32 correct for this batch. total corrected by far=5922/7072\n",
      "24/32 correct for this batch. total corrected by far=5946/7104\n",
      "29/32 correct for this batch. total corrected by far=5975/7136\n",
      "30/32 correct for this batch. total corrected by far=6005/7168\n",
      "28/32 correct for this batch. total corrected by far=6033/7200\n",
      "30/32 correct for this batch. total corrected by far=6063/7232\n",
      "28/32 correct for this batch. total corrected by far=6091/7264\n",
      "26/32 correct for this batch. total corrected by far=6117/7296\n",
      "29/32 correct for this batch. total corrected by far=6146/7328\n",
      "27/32 correct for this batch. total corrected by far=6173/7360\n",
      "24/32 correct for this batch. total corrected by far=6197/7392\n",
      "27/32 correct for this batch. total corrected by far=6224/7424\n",
      "27/32 correct for this batch. total corrected by far=6251/7456\n",
      "30/32 correct for this batch. total corrected by far=6281/7488\n",
      "27/32 correct for this batch. total corrected by far=6308/7520\n",
      "26/32 correct for this batch. total corrected by far=6334/7552\n",
      "27/32 correct for this batch. total corrected by far=6361/7584\n",
      "29/32 correct for this batch. total corrected by far=6390/7616\n",
      "27/32 correct for this batch. total corrected by far=6417/7648\n",
      "25/32 correct for this batch. total corrected by far=6442/7680\n",
      "25/32 correct for this batch. total corrected by far=6467/7712\n",
      "26/32 correct for this batch. total corrected by far=6493/7744\n",
      "29/32 correct for this batch. total corrected by far=6522/7776\n",
      "26/32 correct for this batch. total corrected by far=6548/7808\n",
      "28/32 correct for this batch. total corrected by far=6576/7840\n",
      "27/32 correct for this batch. total corrected by far=6603/7872\n",
      "24/32 correct for this batch. total corrected by far=6627/7904\n",
      "29/32 correct for this batch. total corrected by far=6656/7936\n",
      "26/32 correct for this batch. total corrected by far=6682/7968\n",
      "29/32 correct for this batch. total corrected by far=6711/8000\n",
      "27/32 correct for this batch. total corrected by far=6738/8032\n",
      "25/32 correct for this batch. total corrected by far=6763/8064\n",
      "26/32 correct for this batch. total corrected by far=6789/8096\n",
      "28/32 correct for this batch. total corrected by far=6817/8128\n",
      "29/32 correct for this batch. total corrected by far=6846/8160\n",
      "23/32 correct for this batch. total corrected by far=6869/8192\n",
      "27/32 correct for this batch. total corrected by far=6896/8224\n",
      "26/32 correct for this batch. total corrected by far=6922/8256\n",
      "26/32 correct for this batch. total corrected by far=6948/8288\n",
      "27/32 correct for this batch. total corrected by far=6975/8320\n",
      "27/32 correct for this batch. total corrected by far=7002/8352\n",
      "29/32 correct for this batch. total corrected by far=7031/8384\n",
      "27/32 correct for this batch. total corrected by far=7058/8416\n",
      "29/32 correct for this batch. total corrected by far=7087/8448\n",
      "28/32 correct for this batch. total corrected by far=7115/8480\n",
      "20/20 correct for this batch. total corrected by far=7135/8500\n",
      "0.8394117647058823\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model.eval()\n",
    "correct_normal = 0\n",
    "total = 0\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        # print(len(batch))\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # token_type_ids  = batch['token_type_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(batch)\n",
    "        predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "        # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        this_batch_corrected = (predicted==labels.reshape(-1,1)).sum().item()\n",
    "        correct_normal += this_batch_corrected\n",
    "        print(f'{this_batch_corrected}/{labels.size(0)} correct for this batch. total corrected by far={correct_normal}/{total}')\n",
    "        # correct_normal += (predicted == labels).sum().item()\n",
    "        # break\n",
    "\n",
    "        \n",
    "accuracy = correct_normal/total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9532710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5481, 1: 3019})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = train_data['label']\n",
    "\n",
    "from collections import Counter\n",
    "Counter(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e3dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f31be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a68a1119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5508118956394819"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3019/5481"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f851244",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b22749",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
