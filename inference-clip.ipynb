{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5589d4f9-12c1-4b7d-89fd-f37153b01502",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:26.909008Z",
     "iopub.status.busy": "2024-04-14T16:03:26.908561Z",
     "iopub.status.idle": "2024-04-14T16:03:29.017552Z",
     "shell.execute_reply": "2024-04-14T16:03:29.016624Z",
     "shell.execute_reply.started": "2024-04-14T16:03:26.908980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_from_disk\n",
    "from transformers import CLIPTokenizer, CLIPProcessor, AutoTokenizer\n",
    "from transformers import AutoProcessor, FlavaModel\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ebdc08-dc28-49db-9a4c-244bcc8b7c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:29.019447Z",
     "iopub.status.busy": "2024-04-14T16:03:29.018943Z",
     "iopub.status.idle": "2024-04-14T16:03:29.052234Z",
     "shell.execute_reply": "2024-04-14T16:03:29.051618Z",
     "shell.execute_reply.started": "2024-04-14T16:03:29.019420Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bafcc5-5adf-4ed5-bbdc-963b7f7e295f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:29.111997Z",
     "iopub.status.busy": "2024-04-14T16:03:29.111697Z",
     "iopub.status.idle": "2024-04-14T16:03:29.721713Z",
     "shell.execute_reply": "2024-04-14T16:03:29.720945Z",
     "shell.execute_reply.started": "2024-04-14T16:03:29.111976Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.clip import CLIPClassifier, CLIPProcessDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30d238f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:29.796510Z",
     "iopub.status.busy": "2024-04-14T16:03:29.795936Z",
     "iopub.status.idle": "2024-04-14T16:03:40.018590Z",
     "shell.execute_reply": "2024-04-14T16:03:40.017896Z",
     "shell.execute_reply.started": "2024-04-14T16:03:29.796479Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (image_encoder): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_map): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (text_map): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pre_output): Sequential(\n",
       "    (0): Dropout(p=0.4, inplace=False)\n",
       "    (1): Linear(in_features=1048576, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (cross_entropy_loss): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# model = torch.load('clip_entire_model_added_sigmoid_gradient_clip.pt')\n",
    "model = torch.load('model_output/clip_entire_model_added_sigmoid_gradclip_laion-CLIP-ViT-B-32-laion2B-s34B-b79K-cross.pt',\n",
    "                   map_location=torch.device('cuda'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba55d98f",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:40.020408Z",
     "iopub.status.busy": "2024-04-14T16:03:40.020050Z",
     "iopub.status.idle": "2024-04-14T16:03:40.053440Z",
     "shell.execute_reply": "2024-04-14T16:03:40.052822Z",
     "shell.execute_reply.started": "2024-04-14T16:03:40.020384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ImageCaptioningDataset(Dataset):\n",
    "#     def __init__(self, dataset):\n",
    "\n",
    "#         self.dataset = dataset\n",
    "#         self.image_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "#         self.text_processor = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.dataset[idx]\n",
    "#         # import pdb; pdb.set_trace()\n",
    "#         # encoding = self.processor(images=base64str_to_PILobj(item[\"image\"]), text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "#         # # remove batch dimension\n",
    "#         # encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "#         # # import pdb; pdb.set_trace()\n",
    "#         # encoding['label'] = item['label']\n",
    "#         # encoding['image'] = item[\"image\"]\n",
    "#         # return encoding\n",
    "#         pixel_values = self.image_processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\"),\n",
    "#                                             return_tensors=\"pt\")['pixel_values']\n",
    "#         # caption_output = self.text_processor(item[\"caption\"], \n",
    "#         #                                      padding=True,\n",
    "#         #                                      return_tensors=\"pt\",\n",
    "#         #                                      truncation=True)\n",
    "#         text_output = self.text_processor(item['text'],\n",
    "#                                           padding='max_length', \n",
    "#                                           return_tensors=\"pt\", \n",
    "#                                           truncation=True)\n",
    "#         # pdb.set_trace()\n",
    "#         # print(idx, pixel_values.shape)\n",
    "#         # import pdb; pdb.set_trace()\n",
    "#         label = torch.LongTensor([item['label']])\n",
    "#         # import pdb; pdb.set_trace()\n",
    "#         return {\n",
    "#             'pixel_values': pixel_values,\n",
    "#             'input_ids': text_output['input_ids'],\n",
    "#             'attention_mask': text_output['attention_mask'],\n",
    "#             'labels': label,\n",
    "#             # 'input_ids_caption': caption_output['input_ids'],\n",
    "#             # 'attention_mask_caption': caption_output['attention_mask_caption'],\n",
    "#             'idx_memes': item['id'],\n",
    "#             'image': item['image']\n",
    "#         }\n",
    "\n",
    "# def base64str_to_PILobj(base64_string):\n",
    "#     '''\n",
    "#     Args\n",
    "#     - base64_string (str): based64 encoded representing an image\n",
    "\n",
    "#     Output\n",
    "#     - PIL object (use .show() to display)\n",
    "#     '''\n",
    "#     image_data = base64.b64decode(base64_string)\n",
    "#     img = Image.open(io.BytesIO(image_data))\n",
    "#     #img.show()\n",
    "#     return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c5244e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:40.054623Z",
     "iopub.status.busy": "2024-04-14T16:03:40.054330Z",
     "iopub.status.idle": "2024-04-14T16:03:57.240900Z",
     "shell.execute_reply": "2024-04-14T16:03:57.240098Z",
     "shell.execute_reply.started": "2024-04-14T16:03:40.054602Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "combined = load_from_disk('./processed_data/combined_hateful_memes_dataset')\n",
    "\n",
    "train_data = combined['train']\n",
    "train_dataset = CLIPProcessDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "dev_seen_data = combined['dev_seen']\n",
    "dev_seen_dataset = CLIPProcessDataset(dev_seen_data)\n",
    "dev_seen_loader = DataLoader(dev_seen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "dev_unseen_data = combined['dev_unseen']\n",
    "dev_unseen_dataset = CLIPProcessDataset(dev_unseen_data)\n",
    "dev_unseen_loader = DataLoader(dev_unseen_dataset, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920845e3-5c5f-4acb-9c3f-fdc0d7cec3f6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:57.242923Z",
     "iopub.status.busy": "2024-04-14T16:03:57.242596Z",
     "iopub.status.idle": "2024-04-14T16:03:57.281800Z",
     "shell.execute_reply": "2024-04-14T16:03:57.281145Z",
     "shell.execute_reply.started": "2024-04-14T16:03:57.242898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "device='cuda'\n",
    "\n",
    "def get_result_df(data_loader, sample_n=None, type_='train'):\n",
    "    fina_res_dict_train = {\n",
    "        'pred': [],\n",
    "        'pred_score': [],\n",
    "        'idx': [],\n",
    "        'img': [],\n",
    "        'labels': [],\n",
    "        'type': []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "\n",
    "            output = model(batch, device=device)\n",
    "            predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "            # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "            fina_res_dict_train['pred'].extend(predicted.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict_train['pred_score'].extend(output.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict_train['idx'].extend(batch['idx_memes']) \n",
    "            fina_res_dict_train['labels'].extend(batch['labels'].detach().cpu().numpy().reshape(-1).tolist()) \n",
    "            fina_res_dict_train['img'].extend(batch['image'])\n",
    "            fina_res_dict_train['type'].extend([type_ for _ in range(len(batch['image']))])\n",
    "            \n",
    "            # break\n",
    "            if sample_n is not None and len(fina_res_dict_train['idx'])>=sample_n:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(fina_res_dict_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f43c65-a65a-4abb-8826-3d5cb3ffc410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:03:57.283011Z",
     "iopub.status.busy": "2024-04-14T16:03:57.282717Z",
     "iopub.status.idle": "2024-04-14T16:04:09.959083Z",
     "shell.execute_reply": "2024-04-14T16:04:09.958410Z",
     "shell.execute_reply.started": "2024-04-14T16:03:57.282989Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e943e7f2a8542acb4fa707af307a308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>idx</th>\n",
       "      <th>img</th>\n",
       "      <th>labels</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.585058</td>\n",
       "      <td>75639</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAiYAAAFmCAIAAADWHMbgAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>78134</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAsoAAAMgCAIAAAC1XNNHAA...</td>\n",
       "      <td>0</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.340448</td>\n",
       "      <td>46971</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAARQAAAGQCAIAAADgMwjgAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.853451</td>\n",
       "      <td>27635</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAATkAAAGQCAIAAABakICAAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.454527</td>\n",
       "      <td>19530</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAPAAAAGQCAIAAAAftHorAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>96284</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAiYAAAFzCAIAAACFB1XIAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>57369</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAAP3aGbAA...</td>\n",
       "      <td>0</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>43810</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAzkAAAI6CAIAAACLkxFUAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>89642</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAzkAAAIlCAIAAAB5E6EaAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1</td>\n",
       "      <td>0.796668</td>\n",
       "      <td>03519</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAiYAAAFtCAIAAAC82zYjAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_unseen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred  pred_score    idx  \\\n",
       "0       1    0.585058  75639   \n",
       "1       0    0.001186  78134   \n",
       "2       0    0.340448  46971   \n",
       "3       1    0.853451  27635   \n",
       "4       0    0.454527  19530   \n",
       "..    ...         ...    ...   \n",
       "535     0    0.000075  96284   \n",
       "536     0    0.000484  57369   \n",
       "537     0    0.012439  43810   \n",
       "538     0    0.000270  89642   \n",
       "539     1    0.796668  03519   \n",
       "\n",
       "                                                   img  labels        type  \n",
       "0    iVBORw0KGgoAAAANSUhEUgAAAiYAAAFmCAIAAADWHMbgAA...       1  dev_unseen  \n",
       "1    iVBORw0KGgoAAAANSUhEUgAAAsoAAAMgCAIAAAC1XNNHAA...       0  dev_unseen  \n",
       "2    iVBORw0KGgoAAAANSUhEUgAAARQAAAGQCAIAAADgMwjgAA...       1  dev_unseen  \n",
       "3    iVBORw0KGgoAAAANSUhEUgAAATkAAAGQCAIAAABakICAAA...       1  dev_unseen  \n",
       "4    iVBORw0KGgoAAAANSUhEUgAAAPAAAAGQCAIAAAAftHorAA...       1  dev_unseen  \n",
       "..                                                 ...     ...         ...  \n",
       "535  iVBORw0KGgoAAAANSUhEUgAAAiYAAAFzCAIAAACFB1XIAA...       1  dev_unseen  \n",
       "536  iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAAP3aGbAA...       0  dev_unseen  \n",
       "537  iVBORw0KGgoAAAANSUhEUgAAAzkAAAI6CAIAAACLkxFUAA...       1  dev_unseen  \n",
       "538  iVBORw0KGgoAAAANSUhEUgAAAzkAAAIlCAIAAAB5E6EaAA...       1  dev_unseen  \n",
       "539  iVBORw0KGgoAAAANSUhEUgAAAiYAAAFtCAIAAAC82zYjAA...       1  dev_unseen  \n",
       "\n",
       "[540 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_result_df(dev_unseen_loader, type_='dev_unseen')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46ffa769-fbf9-4b6d-bd2f-fbfc0285b560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:04:09.960387Z",
     "iopub.status.busy": "2024-04-14T16:04:09.960045Z",
     "iopub.status.idle": "2024-04-14T16:04:09.996188Z",
     "shell.execute_reply": "2024-04-14T16:04:09.995599Z",
     "shell.execute_reply.started": "2024-04-14T16:04:09.960363Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450588235294117"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(df['labels'], df['pred_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c29380",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-13T09:42:48.450980Z",
     "iopub.status.busy": "2024-04-13T09:42:48.450612Z",
     "iopub.status.idle": "2024-04-13T09:48:47.504529Z",
     "shell.execute_reply": "2024-04-13T09:48:47.503583Z",
     "shell.execute_reply.started": "2024-04-13T09:42:48.450954Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c23c0771bd4819b6cf330722b85f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_seen completed in 19.24s. total count=500. Accuracy=0.652\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d856fdd2a3ea48eeaed98b75b78853a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_unseen completed 20.18s. total count=540. Accuracy=0.7092592592592593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be96b39947cf4505b7915466c81600ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train completed 319.57s. total count=8500. Accuracy=0.8874117647058823\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'#'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.eval()\n",
    "correct_normal = 0\n",
    "total = 0\n",
    "model = model.to(device)\n",
    "accuracy = {}\n",
    "fina_res_dict = {\n",
    "    'pred': [],\n",
    "    'pred_score': [],\n",
    "    'idx': [],\n",
    "    'img': [],\n",
    "    'labels': [],\n",
    "    'type': []\n",
    "}\n",
    "gen_df = True\n",
    "\n",
    "t0 = time()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_seen_loader):\n",
    "        # print(batch)\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # #token_type_ids  = batch['token_type_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(batch, device=device)\n",
    "        predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "        # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        this_batch_corrected = (predicted==labels.reshape(-1,1)).sum().item()\n",
    "        correct_normal += this_batch_corrected\n",
    "        # print(f'{this_batch_corrected}/{labels.size(0)} correct for this batch. total corrected by far={correct_normal}/{total}')\n",
    "        # break\n",
    "        if gen_df:\n",
    "            fina_res_dict['pred'].extend(predicted.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict['pred_score'].extend(output.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict['idx'].extend(batch['idx_memes']) \n",
    "            fina_res_dict['labels'].extend(batch['labels'].detach().cpu().numpy().reshape(-1).tolist()) \n",
    "            fina_res_dict['img'].extend(batch['image'])\n",
    "            fina_res_dict['type'].extend(['dev_seen' for _ in range(len(batch['image']))])\n",
    "        # break\n",
    "    accuracy['dev_seen'] = correct_normal/total\n",
    "print(f\"dev_seen completed in {(time()-t0):.2f}s. total count={total}. Accuracy={accuracy['dev_seen']}\")\n",
    "\n",
    "correct_normal = 0\n",
    "total = 0\n",
    "t0 = time()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_unseen_loader):\n",
    "        # print(batch)\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # #token_type_ids  = batch['token_type_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(batch, device=device)\n",
    "        predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "        # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        this_batch_corrected = (predicted==labels.reshape(-1,1)).sum().item()\n",
    "        correct_normal += this_batch_corrected\n",
    "        # print(f'{this_batch_corrected}/{labels.size(0)} correct for this batch. total corrected by far={correct_normal}/{total}')\n",
    "        # break\n",
    "        if gen_df:\n",
    "            fina_res_dict['pred'].extend(predicted.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict['pred_score'].extend(output.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict['idx'].extend(batch['idx_memes']) \n",
    "            fina_res_dict['labels'].extend(batch['labels'].detach().cpu().numpy().reshape(-1).tolist()) \n",
    "            fina_res_dict['img'].extend(batch['image'])\n",
    "            fina_res_dict['type'].extend(['dev_unseen' for _ in range(len(batch['image']))])\n",
    "        # break\n",
    "    accuracy['dev_unseen'] = correct_normal/total\n",
    "\n",
    "print(f\"dev_unseen completed {(time()-t0):.2f}s. total count={total}. Accuracy={accuracy['dev_unseen']}\")\n",
    "correct_normal = 0\n",
    "total = 0\n",
    "t0 = time()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        # print(batch)\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # #token_type_ids  = batch['token_type_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(batch, device=device)\n",
    "        predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "        # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        this_batch_corrected = (predicted==labels.reshape(-1,1)).sum().item()\n",
    "        correct_normal += this_batch_corrected\n",
    "        # print(f'{this_batch_corrected}/{labels.size(0)} correct for this batch. total corrected by far={correct_normal}/{total}')\n",
    "\n",
    "        if gen_df:\n",
    "            fina_res_dict['pred'].extend(predicted.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict['pred_score'].extend(output.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict['idx'].extend(batch['idx_memes']) \n",
    "            fina_res_dict['labels'].extend(batch['labels'].detach().cpu().numpy().reshape(-1).tolist()) \n",
    "            fina_res_dict['img'].extend(batch['image'])\n",
    "            fina_res_dict['type'].extend(['train' for _ in range(len(batch['image']))])\n",
    "        # break\n",
    "\n",
    "        # break\n",
    "    accuracy['train'] = correct_normal/total\n",
    "print(f\"train completed {(time()-t0):.2f}s. total count={total}. Accuracy={accuracy['train']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b67866-9c74-4962-bac5-623c2ff79239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T09:48:47.505933Z",
     "iopub.status.busy": "2024-04-13T09:48:47.505592Z",
     "iopub.status.idle": "2024-04-13T09:48:47.580115Z",
     "shell.execute_reply": "2024-04-13T09:48:47.579003Z",
     "shell.execute_reply.started": "2024-04-13T09:48:47.505909Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>idx</th>\n",
       "      <th>img</th>\n",
       "      <th>labels</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.062818</td>\n",
       "      <td>68530</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAhMAAAMgCAIAAAAWWHfEAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.301817</td>\n",
       "      <td>64510</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAiYAAAFyCAIAAABOW4ZtAA...</td>\n",
       "      <td>0</td>\n",
       "      <td>dev_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>65832</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAUAAAAMgCAIAAADdmSyOAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.074875</td>\n",
       "      <td>46971</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAARQAAAGQCAIAAADgMwjgAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>dev_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013474</td>\n",
       "      <td>03798</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAzkAAAIVCAIAAAB9aaa3AA...</td>\n",
       "      <td>0</td>\n",
       "      <td>dev_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9535</th>\n",
       "      <td>0</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>13469</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAzkAAAIlCAIAAAB5E6EaAA...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9536</th>\n",
       "      <td>1</td>\n",
       "      <td>0.945997</td>\n",
       "      <td>72489</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAiYAAAFuCAIAAAA6T0SNAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9537</th>\n",
       "      <td>1</td>\n",
       "      <td>0.936970</td>\n",
       "      <td>13875</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAiYAAAFuCAIAAAA6T0SNAA...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9538</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025401</td>\n",
       "      <td>34520</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAhQAAAMgCAIAAAD0hGy9AA...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>0</td>\n",
       "      <td>0.035844</td>\n",
       "      <td>23164</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAyAAAAMgCAIAAABUEpE/AA...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9540 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred  pred_score    idx  \\\n",
       "0        0    0.062818  68530   \n",
       "1        0    0.301817  64510   \n",
       "2        0    0.032403  65832   \n",
       "3        0    0.074875  46971   \n",
       "4        0    0.013474  03798   \n",
       "...    ...         ...    ...   \n",
       "9535     0    0.028382  13469   \n",
       "9536     1    0.945997  72489   \n",
       "9537     1    0.936970  13875   \n",
       "9538     0    0.025401  34520   \n",
       "9539     0    0.035844  23164   \n",
       "\n",
       "                                                    img  labels      type  \n",
       "0     iVBORw0KGgoAAAANSUhEUgAAAhMAAAMgCAIAAAAWWHfEAA...       1  dev_seen  \n",
       "1     iVBORw0KGgoAAAANSUhEUgAAAiYAAAFyCAIAAABOW4ZtAA...       0  dev_seen  \n",
       "2     iVBORw0KGgoAAAANSUhEUgAAAUAAAAMgCAIAAADdmSyOAA...       1  dev_seen  \n",
       "3     iVBORw0KGgoAAAANSUhEUgAAARQAAAGQCAIAAADgMwjgAA...       1  dev_seen  \n",
       "4     iVBORw0KGgoAAAANSUhEUgAAAzkAAAIVCAIAAAB9aaa3AA...       0  dev_seen  \n",
       "...                                                 ...     ...       ...  \n",
       "9535  iVBORw0KGgoAAAANSUhEUgAAAzkAAAIlCAIAAAB5E6EaAA...       0     train  \n",
       "9536  iVBORw0KGgoAAAANSUhEUgAAAiYAAAFuCAIAAAA6T0SNAA...       1     train  \n",
       "9537  iVBORw0KGgoAAAANSUhEUgAAAiYAAAFuCAIAAAA6T0SNAA...       1     train  \n",
       "9538  iVBORw0KGgoAAAANSUhEUgAAAhQAAAMgCAIAAAD0hGy9AA...       0     train  \n",
       "9539  iVBORw0KGgoAAAANSUhEUgAAAyAAAAMgCAIAAABUEpE/AA...       0     train  \n",
       "\n",
       "[9540 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(fina_res_dict)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deb1ebb1-5eb3-4ed4-b917-913452e0798f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-13T10:09:46.965657Z",
     "iopub.status.busy": "2024-04-13T10:09:46.965130Z",
     "iopub.status.idle": "2024-04-13T10:09:47.175861Z",
     "shell.execute_reply": "2024-04-13T10:09:47.175145Z",
     "shell.execute_reply.started": "2024-04-13T10:09:46.965621Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train AUC: 0.948\n",
      "dev_seen AUC: 0.760\n",
      "dev_unseen AUC: 0.750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_seen': 0.652,\n",
       " 'dev_unseen': 0.7092592592592593,\n",
       " 'train': 0.8874117647058823}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# clip_entire_model_added_sigmoid_gradclip.pt\n",
    "for type_ in ['train', 'dev_seen', 'dev_unseen']:\n",
    "    temp_df = df[df['type']==type_]\n",
    "    auc = roc_auc_score(temp_df['labels'], temp_df['pred_score'])\n",
    "    print('%s AUC: %.3f' % (type_, auc))\n",
    "\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d57a5-a642-436c-8aa8-a248f04161a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03763690-01e3-478e-b1f4-693afcd3ee38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d74b4e-67a0-4e94-912e-a34d6f0d6fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b9d1c-76ff-40ef-8eac-00f807344185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9853cf0-f0c7-4849-a5da-be91fe190c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf96ce-b46c-473b-9f0a-73b18d8db8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a3fc990-d208-4d56-9cbc-64bd022c3666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T04:15:06.636648Z",
     "iopub.status.busy": "2024-04-13T04:15:06.636281Z",
     "iopub.status.idle": "2024-04-13T04:15:06.693982Z",
     "shell.execute_reply": "2024-04-13T04:15:06.693158Z",
     "shell.execute_reply.started": "2024-04-13T04:15:06.636610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train AUC: 0.898\n",
      "dev_seen AUC: 0.707\n",
      "dev_unseen AUC: 0.682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# clip_entire_model_added_sigmoid_gradclip_maplayers_5.pt\n",
    "for type_ in ['train', 'dev_seen', 'dev_unseen']:\n",
    "    temp_df = df[df['type']==type_]\n",
    "    auc = roc_auc_score(temp_df['labels'], temp_df['pred_score'])\n",
    "    print('%s AUC: %.3f' % (type_,auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d85dc1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-13T04:06:00.544600Z",
     "iopub.status.busy": "2024-04-13T04:06:00.544078Z",
     "iopub.status.idle": "2024-04-13T04:06:00.567066Z",
     "shell.execute_reply": "2024-04-13T04:06:00.566271Z",
     "shell.execute_reply.started": "2024-04-13T04:06:00.544570Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train AUC: 0.897\n",
      "dev_seen AUC: 0.728\n",
      "dev_unseen AUC: 0.704\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for type_ in ['train', 'dev_seen', 'dev_unseen']:\n",
    "    temp_df = df[df['type']==type_]\n",
    "    auc = roc_auc_score(temp_df['labels'], temp_df['pred_score'])\n",
    "    print('%s AUC: %.3f' % (type_,auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06caadd-795c-4f35-91c9-f2c2feb4dd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8e629-15a6-4257-8b79-7dc6d5bc62a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1544da-ae6d-4c00-af45-3a1db025962e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8f738-df16-4242-8fb8-3835a17bee40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395aff9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9532710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5481, 1: 3019})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = train_data['label']\n",
    "\n",
    "from collections import Counter\n",
    "Counter(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bacabb54",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/weihong/project/inference-clip.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-hongkong.data.aliyun.com/mnt/workspace/weihong/project/inference-clip.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdir\u001b[39m(train_data\u001b[39m.\u001b[39;49mdata[\u001b[39m75210\u001b[39;49m])\n",
      "File \u001b[0;32m/home/pai/envs/translation_dev/lib/python3.8/site-packages/datasets/table.py:431\u001b[0m, in \u001b[0;36mTable.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtable[i]\n",
      "File \u001b[0;32m/home/pai/envs/translation_dev/lib/python3.8/site-packages/pyarrow/table.pxi:1539\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/pai/envs/translation_dev/lib/python3.8/site-packages/pyarrow/table.pxi:1625\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.column\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/pai/envs/translation_dev/lib/python3.8/site-packages/pyarrow/table.pxi:4294\u001b[0m, in \u001b[0;36mpyarrow.lib.Table._column\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/pai/envs/translation_dev/lib/python3.8/site-packages/pyarrow/array.pxi:592\u001b[0m, in \u001b[0;36mpyarrow.lib._normalize_index\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of bounds"
     ]
    }
   ],
   "source": [
    "dir(train_data.data[75210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e3dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f31be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a68a1119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5508118956394819"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3019/5481"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f851244",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b22749",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation_dev",
   "language": "python",
   "name": "translation_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
