{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViLT Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# ViltModel is a raw model with no heads. Can use to define heads\n",
    "from transformers import ViltProcessor, ViltModel\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "import skimage\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import FloatProgress\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViltImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"size_divisor\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_pad\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViltImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 384\n",
       "  },\n",
       "  \"size_divisor\": 32\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 6057, 2033, 4168, 102]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed = processor(Image.open('./dataset/img/98724.png').convert('RGB'),'funny meme')\n",
    "test_processed['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViltModel(\n",
       "  (embeddings): ViltEmbeddings(\n",
       "    (text_embeddings): TextEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(40, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (patch_embeddings): ViltPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "    )\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViltEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViltLayer(\n",
       "        (attention): ViltAttention(\n",
       "          (attention): ViltSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViltSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViltIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViltOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViltPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No separate text and visual encoders, combbined into one transformer encoder that processes both text and image patches in ViT style\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model with Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViltForClassification(nn.Module):\n",
    "    def __init__(self, vilt_base_model, mapping_num, map_dim, device):\n",
    "        super().__init__()\n",
    "        self.vilt = vilt_base_model\n",
    "        self.mapping_num = mapping_num\n",
    "        self.map_dim = map_dim\n",
    "        self.device = device\n",
    "        self.vilt.to(self.device)\n",
    "        # Update all model parameters to be frozen, except for last few layers of certain parts\n",
    "        for param in self.vilt.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze for last layer of encoder\n",
    "        for param in self.vilt.encoder.layer[11].parameters():\n",
    "            param.requires_grad = True\n",
    "        # Unfreeze for final linear pooler layer\n",
    "        for param in self.vilt.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "        # Series of Relu -> Linear -> Dropout layers\n",
    "        map_layers = [nn.Linear(768, self.map_dim), nn.Dropout(0.2)]\n",
    "        for _ in range(mapping_num):\n",
    "            map_layers.extend([nn.ReLU(), nn.Linear(self.map_dim, self.map_dim), nn.Dropout(0.2)])\n",
    "        self.map_layers = nn.Sequential(*map_layers)\n",
    "        # Final layer, 2 classes hateful or non-hateful\n",
    "        self.final_linear = nn.Linear(self.map_dim, 2)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Unpack the processed image and text and feed to VILT model\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        token_type_ids = batch['token_type_ids'].to(self.device)\n",
    "        attn_mask = batch['attention_mask'].to(self.device)\n",
    "        pixel_vals = batch['pixel_values'].to(self.device)\n",
    "        pixel_mask = batch['pixel_mask'].to(self.device)\n",
    "        # Remove additional dimension at 1, specify to not remove batch during inference\n",
    "        model_outs = self.vilt(input_ids.squeeze(1), attn_mask.squeeze(1), token_type_ids.squeeze(1), pixel_vals.squeeze(1), pixel_mask.squeeze(1))\n",
    "        # Feed model outs to  map_layers\n",
    "        map_outs = self.map_layers(model_outs.pooler_output)\n",
    "        # Final output\n",
    "        output = self.final_linear(map_outs)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers\n",
    "def prepare_model(model):\n",
    "    # Freeze all\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze final layers in ViLT\n",
    "    for param in model.vilt.encoder.layer[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.vilt.layernorm.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.vilt.pooler.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Unfreeze for map layers and final linear layer\n",
    "    for param in model.map_layers.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.final_linear.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, root_dir, jsonl_path, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(self.root_dir, jsonl_path), 'r') as f:\n",
    "            self.jsonl = list(f)\n",
    "        self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.jsonl)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        json_str = self.jsonl[i]\n",
    "        json_loaded = json.loads(json_str)\n",
    "        img_path = os.path.join(self.root_dir, json_loaded['img'])\n",
    "        label = torch.tensor(json_loaded['label'])\n",
    "        caption = json_loaded['text']\n",
    "        img = np.array(Image.open(img_path).convert('RGB')).transpose((2, 0, 1))\n",
    "        img = tv_tensors.Image(img)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        img = np.array(img).transpose((1,2,0))\n",
    "        # Need to truncate, pad to max length to ensure that no dataloader issues happen!\n",
    "        processed = processor(img, caption, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': processed['input_ids'],\n",
    "            'token_type_ids': processed['token_type_ids'],\n",
    "            'attention_mask': processed['attention_mask'],\n",
    "            'pixel_values': processed['pixel_values'],\n",
    "            'pixel_mask': processed['pixel_mask'],\n",
    "            'label':  label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch_sample in tqdm(dataloader):\n",
    "        label = batch_sample['label'].to(device)\n",
    "        pred = model(batch_sample)\n",
    "        loss = loss_fn(pred, label)\n",
    "        # print(total_loss)\n",
    "        correct = (torch.argmax(pred, dim = 1) == label).sum()\n",
    "        acc = correct / label.shape[0]\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Add loss\n",
    "        epoch_loss += loss.item() * label.shape[0]\n",
    "        epoch_acc += acc.item() * label.shape[0]\n",
    "    final_epoch_loss = epoch_loss / len(dataloader.sampler)\n",
    "    final_epoch_acc = epoch_acc / len(dataloader.sampler)\n",
    "    return final_epoch_loss, final_epoch_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune on label directly, ignore caption\n",
    "def finetune_model(model, dataloader, optimizer, loss, epochs, device):\n",
    "    model.to(device)\n",
    "    # Keep track of train metrics\n",
    "    train_loss_ls = []\n",
    "    train_acc_ls = []\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        # Train model\n",
    "        epoch_loss, epoch_acc, model = finetune_one_epoch(model, dataloader, optimizer, loss, device)\n",
    "        train_loss_ls.append(epoch_loss)\n",
    "        train_acc_ls.append(epoch_acc)\n",
    "        print(f\"Epoch {epoch}\\n\")\n",
    "        print(f\"\\ttrain_loss: {epoch_loss}\\n\")\n",
    "        print(f\"\\ttrain_acc: {epoch_acc}\\n\")\n",
    "    return model, train_loss_ls, train_acc_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViltForClassification(\n",
       "  (vilt): ViltModel(\n",
       "    (embeddings): ViltEmbeddings(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768)\n",
       "        (position_embeddings): Embedding(40, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (patch_embeddings): ViltPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViltEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViltPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (map_layers): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=38, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=38, out_features=38, bias=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=38, out_features=38, bias=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=38, out_features=38, bias=True)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (final_linear): Linear(in_features=38, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model = ViltForClassification(model, 3, 38, 'cuda')\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_model(classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, classification_model.parameters()), lr  = 5e-5)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HatefulMemesDataset('./dataset', 'train.jsonl', transforms=v2.Compose([v2.CenterCrop((800, 800))]))\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:46<00:00,  4.28s/it]\n",
      " 10%|█         | 1/10 [04:46<43:00, 286.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "\ttrain_loss: 0.6553485418487998\n",
      "\n",
      "\ttrain_acc: 0.6444705882352941\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:40<00:00,  4.19s/it]\n",
      " 20%|██        | 2/10 [09:27<37:45, 283.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "\n",
      "\ttrain_loss: 0.5996144534559811\n",
      "\n",
      "\ttrain_acc: 0.6472941176558242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:48<00:00,  4.31s/it]\n",
      " 30%|███       | 3/10 [14:16<33:19, 285.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "\n",
      "\ttrain_loss: 0.5617582945543177\n",
      "\n",
      "\ttrain_acc: 0.6782352943139918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:38<00:00,  4.16s/it]\n",
      " 40%|████      | 4/10 [18:54<28:17, 282.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "\n",
      "\ttrain_loss: 0.5322473987972035\n",
      "\n",
      "\ttrain_acc: 0.7272941176751081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:41<00:00,  4.20s/it]\n",
      " 50%|█████     | 5/10 [23:35<23:31, 282.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "\n",
      "\ttrain_loss: 0.4926264838330886\n",
      "\n",
      "\ttrain_acc: 0.7638823532777674\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:43<00:00,  4.22s/it]\n",
      " 60%|██████    | 6/10 [28:18<18:50, 282.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "\n",
      "\ttrain_loss: 0.45623255095762366\n",
      "\n",
      "\ttrain_acc: 0.7917647059384514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:55<00:00,  4.41s/it]\n",
      " 70%|███████   | 7/10 [33:14<14:20, 286.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "\n",
      "\ttrain_loss: 0.42330831261242136\n",
      "\n",
      "\ttrain_acc: 0.82035294156916\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:12<00:00,  4.66s/it]\n",
      " 80%|████████  | 8/10 [38:26<09:49, 294.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "\n",
      "\ttrain_loss: 0.3896473267779631\n",
      "\n",
      "\ttrain_acc: 0.8443529413728152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [04:53<00:00,  4.38s/it]\n",
      " 90%|█████████ | 9/10 [43:19<04:54, 294.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "\n",
      "\ttrain_loss: 0.35273694586753845\n",
      "\n",
      "\ttrain_acc: 0.867411764902227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [05:18<00:00,  4.75s/it]\n",
      "100%|██████████| 10/10 [48:38<00:00, 291.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "\n",
      "\ttrain_loss: 0.3234019802458146\n",
      "\n",
      "\ttrain_acc: 0.8827058826334336\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuned_model, train_losses, train_accs = finetune_model(classification_model, train_dataloader, optimizer, loss, 10, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_capability(model, jsonl_path):\n",
    "    labels = []\n",
    "    model_probs = []\n",
    "    model_preds = []\n",
    "    # 0 -> Non-hateful, 1 -> Hateful\n",
    "    with open(jsonl_path, 'r') as json_f:\n",
    "        json_list = list(json_f)\n",
    "    for json_str in tqdm(json_list):\n",
    "        result = json.loads(json_str)\n",
    "        img_path, label, caption = result['img'], result['label'], result['text']\n",
    "        labels.append(label)\n",
    "        # Read image\n",
    "        batch = processor(Image.open(os.path.join('./dataset', img_path)).convert('RGB'), caption, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            out = model(batch)\n",
    "            probs = out.softmax(dim=-1).cpu().numpy()\n",
    "            class_1_prob = probs[0][1]\n",
    "            model_probs.append(class_1_prob)\n",
    "            model_preds.append(np.argmax(probs))\n",
    "    return roc_auc_score(labels, model_probs), accuracy_score(labels, model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model_capability(finetuned_model, './dataset/dev_seen.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [00:17<00:00, 31.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6508088235294118, 0.6592592592592592)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_capability(finetuned_model, './dataset/dev_unseen.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:00<00:00, 33.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6675093333333333, 0.6815)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_capability(finetuned_model, './dataset/test_unseen.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(finetuned_model.state_dict(), 'models/vilt-b32-mlm_map_num_3_10_epoch_5e-5.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hateful_memes_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
