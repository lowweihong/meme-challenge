{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da40d644-7443-4e0d-9b9f-91b4b705ef45",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "def base64str_to_PILobj(base64_string):\n",
    "    '''\n",
    "    Args\n",
    "    - base64_string (str): based64 encoded representing an image\n",
    "\n",
    "    Output\n",
    "    - PIL object (use .show() to display)\n",
    "    '''\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(image_data))\n",
    "    #img.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4eaddd3-5d18-4dcf-a62c-c1ab9c75234f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_result_df(data_loader, model, sample_n=None, type_='train'):\n",
    "    fina_res_dict_train = {\n",
    "        'pred': [],\n",
    "        'pred_score': [],\n",
    "        'idx': [],\n",
    "        'img': [],\n",
    "        'labels': [],\n",
    "        'type': []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "\n",
    "            output = model(batch, device=device)\n",
    "            predicted = torch.as_tensor((output - 0.5) > 0, dtype=torch.int32)\n",
    "            # _, predicted = nn.sigmoid(output)#torch.max(output.data, 1)\n",
    "\n",
    "            fina_res_dict_train['pred'].extend(predicted.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict_train['pred_score'].extend(output.detach().cpu().numpy().reshape(-1).tolist())\n",
    "            fina_res_dict_train['idx'].extend(batch['idx_memes']) \n",
    "            fina_res_dict_train['labels'].extend(batch['labels'].detach().cpu().numpy().reshape(-1).tolist()) \n",
    "            fina_res_dict_train['img'].extend(batch['image'])\n",
    "            fina_res_dict_train['type'].extend([type_ for _ in range(len(batch['image']))])\n",
    "            \n",
    "            # break\n",
    "            if sample_n is not None and len(fina_res_dict_train['idx'])>=sample_n:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(fina_res_dict_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6150a-2c3e-4e5a-a704-6075fbd3e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6eaaa9-5de8-4990-aa91-0036ce6f7fd3",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "res_map = {\n",
    "    # 'clip_entire_model_added_sigmoid_gradclip.pt':{\n",
    "    #     'model_':'CLIPModel',\n",
    "    #     'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "    #     'param': {\n",
    "    #         'epo': 15,\n",
    "    #         'head': 'concat',\n",
    "    #         'map_dim': 32,\n",
    "    #         'batch_size': 16,\n",
    "    #         'po_layer': 1\n",
    "    #     }\n",
    "    # },\n",
    "    'clip_entire_model_added_sigmoid_gradclip.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 15,\n",
    "            'head': 'concat',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 1\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip-cross.pt':{ #BEST\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'cross',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 5\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip_laion-CLIP-ViT-B-32-laion2B-s34B-b79K-cross.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'cross',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 5\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip-att-layer5.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'self-att',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 5\n",
    "        }\n",
    "    },\n",
    "    'clip_entire_model_added_sigmoid_gradclip-cross-layer10.pt':{\n",
    "        'model_':'CLIPModel',\n",
    "        'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "        'param': {\n",
    "            'epo': 20,\n",
    "            'head': 'cross',\n",
    "            'map_dim': 1024,\n",
    "            'batch_size': 64,\n",
    "            'po_layer': 10\n",
    "        }\n",
    "    },\n",
    "    # 'clip_entire_model_added_sigmoid_gradclip-cross-unfreeze-last-block.pt':{\n",
    "    #     'model_':'CLIPModel',\n",
    "    #     'pretrained_model': \"openai/clip-vit-large-patch14\",\n",
    "    #     'param': {\n",
    "    #         'epo': 20,\n",
    "    #         'head': 'cross',\n",
    "    #         'map_dim': 1024,\n",
    "    #         'batch_size': 8,\n",
    "    #         'po_layer': 5\n",
    "    #     }\n",
    "    # }\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6eb7ed4-7ca7-4b1b-a66f-8eb46a02e7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined = load_from_disk('./processed_data/combined_hateful_memes_dataset')\n",
    "\n",
    "train_data = combined['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51c4e15-7820-4047-8716-9c357e63eb75",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPProcessDataset(Dataset):\n",
    "    def __init__(self, dataset, pretrain_model):\n",
    "        self.image_size = 224\n",
    "        self.dataset = dataset\n",
    "        self.image_processor = CLIPProcessor.from_pretrained(pretrain_model)\n",
    "        self.text_processor = CLIPTokenizer.from_pretrained(pretrain_model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        pixel_values = self.image_processor(images=base64str_to_PILobj(item[\"image\"]).convert(\"RGB\").resize((self.image_size, self.image_size)),\n",
    "                                            return_tensors=\"pt\")['pixel_values']\n",
    "\n",
    "        text_output = self.text_processor(item['text'],\n",
    "                                          padding='max_length', \n",
    "                                          return_tensors=\"pt\", \n",
    "                                          truncation=True)\n",
    "        label = torch.LongTensor([item['label']])\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': text_output['input_ids'],\n",
    "            'attention_mask': text_output['attention_mask'],\n",
    "            'labels': label,\n",
    "            'idx_memes': item['id'],\n",
    "            'image': item['image']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42be43b7-b37a-4cf8-be15-78014b98e2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "clip_entire_model_added_sigmoid_gradclip.pt\n",
      "dev_unseen 0.7605147058823529 0.7018518518518518\n",
      "dev_seen 0.7700948936646876 0.69\n",
      "test_unseen 0.7846026666666668 0.7325\n",
      "test_seen 0.8000560224089636 0.731\n",
      "==========\n",
      "clip_entire_model_added_sigmoid_gradclip-cross.pt\n",
      "dev_unseen 0.8117941176470589 0.7574074074074074\n",
      "dev_seen 0.8291433966491176 0.696\n",
      "test_unseen 0.8240634666666667 0.756\n",
      "test_seen 0.8336214485794315 0.729\n",
      "==========\n",
      "clip_entire_model_added_sigmoid_gradclip_laion-CLIP-ViT-B-32-laion2B-s34B-b79K-cross.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd28fa1a0c2a4fc49ca46dfd02b01864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b809dc9de38e44248f1a71fa7d46d00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426c4762ad924f20913ed234c2637b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc245feb88942ce81155d6573dbc22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9658dd65ee04b4bb71f29c1ae65d8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f880faca9298474c9d94b5e8d7949419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_unseen 0.7381470588235294 0.6907407407407408\n",
      "dev_seen 0.753900561680882 0.67\n",
      "test_unseen 0.7659136 0.7325\n",
      "test_seen 0.7693477390956383 0.691\n",
      "==========\n",
      "clip_entire_model_added_sigmoid_gradclip-att-layer5.pt\n",
      "dev_unseen 0.5291323529411764 0.6296296296296297\n",
      "dev_seen 0.4927109503768543 0.506\n",
      "test_unseen 0.5095573333333333 0.625\n",
      "test_seen 0.5049779911964786 0.51\n",
      "==========\n",
      "clip_entire_model_added_sigmoid_gradclip-cross-layer10.pt\n",
      "dev_unseen 0.7653823529411765 0.7388888888888889\n",
      "dev_seen 0.7817765758269191 0.692\n",
      "test_unseen 0.7662570666666667 0.733\n",
      "test_seen 0.7825490196078432 0.696\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "device='cuda'\n",
    "\n",
    "\n",
    "for fp, map_dict in res_map.items():\n",
    "    print('==========')\n",
    "    print(fp)\n",
    "    model = torch.load(f'model_output/{fp}')\n",
    "    \n",
    "    # Generate process DataSet\n",
    "    processer = CLIPProcessDataset\n",
    "    train_dataset = processer(train_data, map_dict['pretrained_model'])\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    dev_seen_data = combined['dev_seen']\n",
    "    dev_seen_dataset = processer(dev_seen_data, map_dict['pretrained_model'])\n",
    "    dev_seen_loader = DataLoader(dev_seen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    dev_unseen_data = combined['dev_unseen']\n",
    "    dev_unseen_dataset = processer(dev_unseen_data, map_dict['pretrained_model'])\n",
    "    dev_unseen_loader = DataLoader(dev_unseen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    test_seen_data = combined['test_seen']\n",
    "    test_seen_dataset = processer(test_seen_data, map_dict['pretrained_model'])\n",
    "    test_seen_loader = DataLoader(test_seen_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    test_unseen_data = combined['test_unseen']\n",
    "    test_unseen_dataset = processer(test_unseen_data, map_dict['pretrained_model'])\n",
    "    test_unseen_loader = DataLoader(test_unseen_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    data_dict = {\n",
    "        'dev_unseen': dev_unseen_loader,\n",
    "        'dev_seen': dev_seen_loader,\n",
    "        'test_unseen': test_unseen_loader,\n",
    "        'test_seen': test_seen_loader,\n",
    "    }\n",
    "    \n",
    "    for type_, type_loader in data_dict.items():\n",
    "        df = get_result_df(type_loader, model, type_=type_)\n",
    "\n",
    "        print(type_, roc_auc_score(df['labels'], df['pred_score']), accuracy_score(df['labels'], df['pred']))\n",
    "    \n",
    "    del model\n",
    "    # print('==========')\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c6bad-9f08-41c1-903d-72f01fd50dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
